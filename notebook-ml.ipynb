{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"0b273c37ffabc846887c9c57ad259dadbe4b7d60fa7973c931ec1b13f33c760d"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let the challenge begin\n\n**Notes on data** \n\n- 5 EEG derivations sampled at 250Hz\n- 3 Accelerometers derivations sampled at 50Hz\n- Sleep epoch = 30 sec\n- hypnogram = succession of the sleep stages (0...5)\n\n**General info sleep**\n- Sleep stages = (N1, N2) = light sleep, N3 = deep sleep, REM\n- Low frequency power: N3 > N2 > N1-REM-Wake\n\n**Wake**\n- During Wake epoch alpha waves are clearly visible on the F-O derivation\n- Movement occured mainly during wake periods, noisy signals during movement\n- Alpha wave frequency ranges between 8 and 13 hertz = wake, relaxed\n**N1**\n- Theta waves freq betw 4 and 8 Hz = N1, N2\n\n**N2**\n- On N2 epoch, power in the spindle range is much higher on frontal-frontal channels\n- Theta waves freq betw 4 and 8 Hz = N1, N2\n- During N2, sleep spindles (fast rythm between 12-14Hz which last between 0.5 up to 2 seconds) are more visible on the Frontal-frontal derivation\n\n**N3**\n- On N3 epoch, we can see more power in the low frequencies\n- Delta waves freq betw 1 and 4 Hz = N3\n\n**REM**\n- REM sleep distinguishable with steady EEG and eyes movement which can be seen when looking at Frontal-occipital vs frontal-frontal derivation.\n- The EEG power increases in the low-frequency band when the sleep stage change from REM to NREM sleep stages\n- REM epoch have more steady EEG\n\n**Formulas**\n- Spectrogram are the time-frequency matrix z = P(t, f)\n- Spectrum correspond to the curves y = P(frequency)\n- Average Spectrum can therefore be computed as the mean of spectromgram over a specified period \n\n**Links**\nhttps://opentext.wsu.edu/psych105/chapter/stages-of-sleep/\nhttps://www.sleepfoundation.org/how-sleep-works/alpha-waves-and-sleep\nhttps://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/Challenge%20Data%20Dreem-1.pdf\nhttps://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/entropy-18-00272.pdf\n\n","metadata":{}},{"cell_type":"code","source":"!pip install lspopt\n!pip install skorch\n!pip install cuda-python\n\ndirectory = '/kaggle/input/files-ml/'\nout_directory = '/kaggle/working/'\n\n#directory = './'\n#out_directory = './'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"directory = '/kaggle/input/files-ml/'\nout_directory = '/kaggle/working/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nfrom matplotlib.colors import Normalize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom os import listdir\nfrom random import randint\nimport random as rd\nimport os \n\nfrom lspopt import spectrogram_lspopt\nfrom scipy.signal import spectrogram\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom sklearn.metrics import plot_confusion_matrix, f1_score\nfrom sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torchaudio.transforms as T\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import ParameterGrid, GridSearchCV\n\nimport joblib","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"frequency_bands = {\n        \"delta\": [0.5, 4],\n        \"theta\": [4, 8],\n        \"alpha\": [8, 12],\n       \"sigma\": [12, 16],\n       \"beta\": [16, 30]\n    }\n\nEEG_FS = 250\nACC_FS = 50 \nepoch_s = 30\nn_EEG = 5\nn_ACC = 3\n\nhypnograms = pd.read_csv(directory + 'targets_train.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(plt.imread(directory + '/corr_sleep_stages.png'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Functions ","metadata":{}},{"cell_type":"code","source":"def get_average_spectrum_for_epochs(eeg,epochs):\n    \"\"\"\n    Return the average power in each of the fourier bin for several epochs.\n    \"\"\"\n    psds = []\n    for epoch in epochs:\n        idx_start,idx_end = 250 * 30 * epoch,250 * 30 * (epoch + 1)\n        freqs,t,psd = spectrogram_lspopt(np.clip(eeg[idx_start:idx_end],-150,150),250,nperseg = 1000)\n        psds += [np.mean(psd ** 2,1)]\n    return freqs,np.array(psds).mean(0)\n\n\n## Function to plot N sleep epochs for a specific stage\n\ndef random_sleep_epoch(N, sleep_stage, hypnogram) :\n    k = 0\n    a = randint(0,len(hypnogram))\n    epochs = []\n    while k < N:\n        if hypnogram[a] == sleep_stage :\n            epochs.append(a)\n            k += 1\n            a = randint(0,len(hypnogram))\n        else :\n            a = randint(0,len(hypnogram))\n    eeg_ff = np.load('sample/sample/f8_f7.npy')\n    for epoch in epochs : \n        t0 = epoch*epoch_s*EEG_FS\n        eeg_short = eeg_ff[t0:t0+(epoch_s*EEG_FS)]\n        plt.figure(figsize=(25, 8))\n        plt.plot(eeg_short)\n        plt.ylim([-200, 200])\n        plt.xlim(0,len(eeg_short))\n        plt.show()\n\ndef plot_spectrogram(\n    data,\n    sf,\n    hypno=None,\n    win_sec=30,\n    fmin=0.5,\n    fmax=25,\n    trimperc=2.5,\n    cmap=\"RdBu_r\",\n    vmin=None,\n    vmax=None,\n):\n    # Safety checks\n    assert isinstance(data, np.ndarray), \"Data must be a 1D NumPy array.\"\n    assert isinstance(sf, (int, float)), \"sf must be int or float.\"\n    assert data.ndim == 1, \"Data must be a 1D (single-channel) NumPy array.\"\n    assert isinstance(win_sec, (int, float)), \"win_sec must be int or float.\"\n    assert isinstance(fmin, (int, float)), \"fmin must be int or float.\"\n    assert isinstance(fmax, (int, float)), \"fmax must be int or float.\"\n    assert fmin < fmax, \"fmin must be strictly inferior to fmax.\"\n    assert fmax < sf / 2, \"fmax must be less than Nyquist (sf / 2).\"\n    assert isinstance(vmin, (int, float, type(None))), \"vmin must be int, float, or None.\"\n    assert isinstance(vmax, (int, float, type(None))), \"vmax must be int, float, or None.\"\n    if vmin is not None:\n        assert isinstance(vmax, (int, float)), \"vmax must be int or float if vmin is provided\"\n    if vmax is not None:\n        assert isinstance(vmin, (int, float)), \"vmin must be int or float if vmax is provided\"\n\n    # Calculate multi-taper spectrogram\n    nperseg = int(win_sec * sf)\n    assert data.size > 2 * nperseg, \"Data length must be at least 2 * win_sec.\"\n    f, t, Sxx = spectrogram_lspopt(data, sf, nperseg=nperseg, noverlap=0)\n    Sxx = 10 * np.log10(Sxx)  # Convert uV^2 / Hz --> dB / Hz\n\n    # Select only relevant frequencies (up to 30 Hz)\n    good_freqs = np.logical_and(f >= fmin, f <= fmax)\n    Sxx = Sxx[good_freqs, :]\n    f = f[good_freqs]\n    t /= 3600  # Convert t to hours\n\n    # Normalization\n    if vmin is None:\n        vmin, vmax = np.percentile(Sxx, [0 + trimperc, 100 - trimperc])\n        norm = Normalize(vmin=vmin, vmax=vmax)\n    else:\n        norm = Normalize(vmin=vmin, vmax=vmax)\n\n    if hypno is None:\n        fig, ax = plt.subplots(nrows=1, figsize=(3, 1))\n        im = ax.pcolormesh(t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\")\n        ax.set_xlim(0, t.max())\n        plt.setp([ax.get_xticklines() + ax.get_yticklines() + ax.get_xgridlines() + ax.get_ygridlines()],antialiased=False)\n        mpl.rcParams['text.antialiased']=False\n        fig.tight_layout(pad=0)\n        return fig\n\ndef split_train_test(records, split=0.5) : \n    rd.seed(1234)\n    rd.shuffle(records)\n    k = int(len(records)*split)\n    return records[:k],records[k:]\n\n\ndef spectral_power(data,n,fs) :\n    for i in range (n) :\n        sfreqs,t,psd = spectrogram(data[:,i,:], fs, nperseg = 1000,noverlap = 750)\n        psd = np.mean(np.abs(psd),-1)\n        l = []\n        for name, freqband in frequency_bands.items():\n            spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n            spec_power = np.sum(spec_power, 1)\n            l.append(spec_power / np.sum(psd,1))\n        matrice = np.array(l) \n        matrice = np.vstack((matrice, np.array([np.mean(data[k,i,:]) for k in range (len(data))]).T))\n        matrice = np.vstack((matrice, np.array([np.std(data[k,i,:]) for k in range (len(data))]).T))\n        if i == 0:\n            complete_array = matrice \n        else :\n            complete_array = np.vstack((complete_array,matrice))\n    return(complete_array.T)\n\n\ndef only_stats(data,n) : \n    for i in range (n) :\n        matrice = np.array([np.mean(data[k,i,:]) for k in range (len(data))]).T\n        matrice = np.vstack((matrice, np.array([np.std(data[k,i,:]) for k in range (len(data))]).T))\n        if i == 0:\n            complete_array = matrice \n        else :\n            complete_array = np.vstack((complete_array,matrice))\n    return(complete_array.T)\n\n\ndef correlations(record, data, n) :\n    corr = [0]*n\n    for i in range (n) :\n        corr[i] = [0]*n\n        for j in range (n) :\n            corr[i][j] = np.corrcoef(data[record,i,:], data[record,j,:])[0][1]\n        print(i+1,corr[i]) \n    return corr  \n\ndef whitening(X):  \n    cov = np.cov(X)\n    d, E = np.linalg.eigh(cov)\n    D = np.diag(d)\n    D_inv = np.sqrt(np.linalg.inv(D))\n    X_whiten = np.dot(E, np.dot(D_inv, np.dot(E.T, X)))\n    return X_whiten\n\ndef normalize_data(eeg_array):\n    \"\"\"normalize signal between 0 and 1\"\"\"\n\n    normalized_array = np.clip(eeg_array, -250, 250)\n    normalized_array = normalized_array / 250\n\n    return normalized_array\n\n\nclass EegEpochDataset(Dataset):\n    \"\"\"EEG Epochs dataset.\"\"\"\n\n    def __init__(self, x_data, y_data, transform=None):\n        \"\"\"\n        Args:\n            x_data (numpy array): Numpy array of input data.\n            y_data (list of numpy array): Sleep Stages\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.y_data = y_data\n        self.x_data = x_data\n        self.transform = transform\n\n        self.x_data = normalize_data(x_data)\n\n    def __len__(self):\n        return len(self.y_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        signal = np.expand_dims(self.x_data[idx], axis=0)\n        stage = self.y_data[idx]\n\n        if self.transform:\n            signal = self.transform(signal)\n\n        return signal, stage\n    \nclass EegEpochDataset_test(Dataset):\n    \"\"\"EEG Epochs dataset.\"\"\"\n\n    def __init__(self, x_data, transform=None):\n        \"\"\"\n        Args:\n            x_data (numpy array): Numpy array of input data.\n            y_data (list of numpy array): Sleep Stages\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.x_data = x_data\n        self.transform = transform\n\n        self.x_data = normalize_data(x_data)\n        \n    def __len__(self):\n        return len(self.x_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        signal = np.expand_dims(self.x_data[idx], axis=0)\n\n        if self.transform:\n            signal = self.transform(signal)\n\n        return signal\n\n\nclass EarlyStopper:\n    def __init__(self, patience=1, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_validation_loss = np.inf\n\n    def early_stop(self, validation_loss):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.counter = 0\n        elif validation_loss > (self.min_validation_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight.data)\n\nclass SingleChannelConvNet(nn.Module):\n\n    def __init__(self):\n        super(SingleChannelConvNet, self).__init__()\n        self.conv_a = nn.Conv1d(1, 8, 5, stride=5)\n        self.batchnorm_a = nn.BatchNorm1d(8, eps=0.001, momentum=0.99)\n        self.conv_b = nn.Conv1d(8, 32, 5, stride=3)\n        self.batchnorm_b = nn.BatchNorm1d(32, eps=0.001, momentum=0.99)\n        self.conv_c = nn.Conv1d(32, 64, 3, stride=3)\n        self.batchnorm_c= nn.BatchNorm1d(64, eps=0.001, momentum=0.99)\n        self.conv_d = nn.Conv1d(64, 128, 3, stride=2)\n        self.batchnorm_d= nn.BatchNorm1d(128, eps=0.001, momentum=0.99)\n        self.conv_e = nn.Conv1d(128, 256, 3, stride=1)\n        self.batchnorm_e= nn.BatchNorm1d(256, eps=0.001, momentum=0.99)\n       \n        self.max_pool_a = nn.MaxPool1d(5, stride=1)\n        self.max_pool_prime = nn.MaxPool1d(3, stride=1)\n        self.dropout = nn.Dropout(p=0.4)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(118784, 256)\n        self.fc2 = nn.Linear(256, 5)\n        self.fc_lin = nn.Linear(256,5)\n\n    def forward(self, x):\n        x = self.relu(self.batchnorm_a(self.conv_a(x)))\n        # x = self.max_pool_a(x)\n        \n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_b(self.conv_b(x)))\n        # x = self.max_pool_prime(x)\n        x = self.dropout(x)\n        x = self.relu(self.batchnorm_c(self.conv_c(x)))\n        # x = self.max_pool_prime(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_d(self.conv_d(x)))\n        # x = self.max_pool_prime(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_e(self.conv_e(x)))\n        # x = self.max_pool_prime(x)\n        x = self.dropout(x)\n        x = x.max(-1)[0]\n        # x = self.softmax(x)\n        # x = torch.flatten(x,1)\n        # x = self.fc1(x)\n        # x = self.fc2(x)\n        x = self.fc_lin(x)\n        # x = F.log_softmax(x, dim=1) #at the moment the softmax is bad\n        # x = self.avg_pool(x)\n        # x = self.max_pool(x)\n\n        return x\n\nclass BestperformingConvNet(nn.Module):\n\n    def __init__(self):\n        super(BestperformingConvNet, self).__init__()\n        self.conv_a = nn.Conv1d(1, 8, 5, stride=5)\n        self.batchnorm_a = nn.BatchNorm1d(8, eps=0.001, momentum=0.99)\n        self.conv_b = nn.Conv1d(8, 32, 5, stride=3)\n        self.batchnorm_b = nn.BatchNorm1d(32, eps=0.001, momentum=0.99)\n        self.conv_c = nn.Conv1d(32, 64, 3, stride=3)\n        self.batchnorm_c= nn.BatchNorm1d(64, eps=0.001, momentum=0.99)\n        self.conv_d = nn.Conv1d(64, 128, 3, stride=2)\n        self.batchnorm_d= nn.BatchNorm1d(128, eps=0.001, momentum=0.99)\n        self.conv_e = nn.Conv1d(128, 256, 3, stride=1)\n        self.batchnorm_e= nn.BatchNorm1d(256, eps=0.001, momentum=0.99)\n\n        self.max_pool_a = nn.MaxPool1d(5, stride=1)\n        self.max_pool_prime = nn.MaxPool1d(3, stride=1)\n        self.dropout = nn.Dropout(p=0.2)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(117760, 128)\n        self.fc2 = nn.Linear(128, 5)\n        self.fc_lin = nn.Linear(256,5)\n\n    def forward(self, x):\n\n        x = self.relu(self.batchnorm_a(self.conv_a(x)))\n        #x = self.dropout(x)\n        x = self.relu(self.batchnorm_b(self.conv_b(x)))\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_c(self.conv_c(x)))\n        x = self.max_pool_prime(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_d(self.conv_d(x)))\n        #x = self.dropout(x)\n        x = self.relu(self.batchnorm_e(self.conv_e(x)))\n        x = self.dropout(x)\n        x = x.max(-1)[0]\n        x = self.fc_lin(x)\n        x = self.max_pool_prime(x)\n        # x = F.log_softmax(x, dim=1) #at the moment the softmax is bad\n\n        return x\n\n\nclass SpectrogramCNN(nn.Module):\n\n    def __init__(self):\n        super(SpectrogramCNN, self).__init__()\n        self.conv_a = nn.Conv2d(1, 8, 25, stride=3)\n        self.conv_b = nn.Conv2d(8, 16, 15, stride=2)\n        self.conv_c = nn.Conv2d(16, 64, 5, stride=1)\n        self.conv_d = nn.Conv2d(64, 128, 2, stride=1)\n        self.conv_e = nn.Conv2d(128, 256, 2, stride=1)\n       \n        self.MP_prime= nn.MaxPool2d(5, stride=1)\n        self.MP_ultra = nn.MaxPool2d(5, stride = 5)\n        \n        self.dropout1 = nn.Dropout(p=0.2)\n        self.dropout2 = nn.Dropout(p=0.2)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n        self.relu = nn.ReLU()\n\n        self.fc1 = nn.Linear(19200, 128)\n        self.fc2 = nn.Linear(128, 5)\n\n    def forward(self, x):\n        transf = T.Spectrogram(n_fft=528, win_length=None, hop_length=int(EEG_FS * 150e-3), center=True, pad_mode=\"reflect\",power=2.0)\n        x = transf(x)\n        x = self.conv_a(x)\n        x = self.relu(x)\n        # x = self.MP_prime(x)\n        x = self.dropout1(x)\n        x = self.conv_b(x)\n        x = self.relu(x)\n        x = self.dropout2(x)\n        x = self.MP_prime(x)\n        x = self.relu(self.conv_c(x))\n        x = self.dropout1(x)\n        x = self.relu(self.conv_d(x))\n        x = self.MP_ultra(x)\n        # x = x.max(-1)[0]\n\n        x = torch.flatten(x, 1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n\ndef make_data_set(record):\n    if type(record) == list:\n        data = np.load(f'{directory}training_records/{record[0]}')\n        record_number = int(record[0][-5])\n        hypnogram = list(hypnograms[hypnograms['record'] == record_number]['target'])\n        for r in range(1,len(record)):\n            record_number = int(record[r][-5])\n            h = list(hypnograms[hypnograms['record'] == record_number]['target'])\n            hypnogram.extend(h)\n            d = np.load(f'{directory}training_records/{record[r]}')\n            data = np.vstack((data,d))\n    else:\n        data = np.load(f'{directory}training_records/{record}')\n        record_number = int(record[-5])\n        hypnogram = list(hypnograms[hypnograms['record'] == record_number]['target'])\n    return(data,hypnogram)\n\ndef make_data_set_test(record):\n    if type(record) == list:\n        data = np.load(f'{directory}test_records/{record[0]}')\n        record_number = int(record[0][-5])\n        record = [record[0]]\n        for r in range(1,len(record)):\n            record_number = int(record[r][-5])\n            d = np.load(f'{directory}test_records/{record[r]}')\n            data = np.vstack((data,d))\n            record.append(record[r])\n    else:\n        data = np.load(f'{directory}test_records/{record}')\n        record = [record]\n    return(data)\n\ndef EEG_ACC(data) :\n    EEG = data[:,1:EEG_FS * epoch_s * n_EEG + 1]\n    EEG = EEG.reshape(len(data), n_EEG, EEG_FS * epoch_s)\n    ACC = data[:,EEG_FS * epoch_s * n_EEG + 1:]\n    ACC = ACC.reshape(len(data), n_ACC, ACC_FS * epoch_s)\n    return EEG, ACC\n\ndef EEG_spectral(EEG) :\n    EEG_spectral_power = spectral_power(EEG,n_EEG,EEG_FS)\n    return EEG_spectral_power\n\ndef ACC_statistics(ACC):\n    return only_stats(ACC,n_ACC)\n\ndef image_spectrale(EEG, eeg_i) :\n    EEG_i = EEG[:,eeg_i,:]\n    images = [0]*len(EEG_i)\n    for i,eeg in enumerate(EEG_i) :\n        fig = plot_spectrogram(np.clip(eeg,-200,200), 250, cmap='Spectral_r', win_sec = 5,trimperc = 2)\n        fig.canvas.draw ()\n        mat = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n        mat = mat.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        images[i] = mat\n        plt.close()\n    return np.array(images)\n    \nclass spectral_img(BaseEstimator, TransformerMixin) :\n    def __init__(self,EEG_i = 0) :\n        self.EEG_i = EEG_i \n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG,_ = EEG_ACC(x)\n        self.img = image_spectrale(EEG, self.EEG_i)\n        n =self.img.shape[1]*self.img.shape[2]*self.img.shape[3]\n        return self.img.reshape(self.img.shape[0], n).astype('float32')\n\nclass EEG_unique(BaseEstimator, TransformerMixin) : \n    def __init__(self,EEG_i = 0) :\n        self.EEG_i = EEG_i \n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG,_ = EEG_ACC(x)\n        return EEG[:,self.EEG_i,:]\n\nclass spectral_and_acc(BaseEstimator, TransformerMixin) :\n    def __init__(self) :\n        return None\n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG, ACC = EEG_ACC(x)\n        self.EEG_spectral_power = EEG_spectral(EEG)\n        self.ACC_stats = ACC_statistics(ACC)\n        EEG_and_ACC_spectral_power = np.vstack(((self.EEG_spectral_power).T, (self.ACC_stats).T))\n        return EEG_and_ACC_spectral_power.T\n\n\nclass CNN_code(ClassifierMixin, BaseEstimator) :\n\n    def __init__(self):\n        self.net = BestperformingConvNet()\n        # device: use GPU if available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.net = self.net.to(self.device) # model into GP\n        self.n_epoch = 70\n        self.min_validation_loss = np.inf\n        self.min_validation_error = np.inf\n        self.n_splits = 12\n        self.learning_rate = 1e-3\n        \n    \n    def fit(self, x, y) :\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.net.parameters())\n        x, y = check_X_y(x, y)\n        self.classes_ = unique_labels(y)\n        self.X_ = x\n        self.y_ = y\n        self.net.train()\n        print('training...')\n\n        #KFold setting\n        KCV = KFold(n_splits=self.n_splits) #shuffle=True, random_state=15011999 \n        self.val_error = []\n        self.training_error = []\n        for fold, (train_index, val_index) in enumerate(KCV.split(x)):\n            print('------------fold no---------{}----------------------'.format(fold+1))\n            # weight resetting: avoid weight leakage\n            self.net.apply(weights_init)\n\n            early_stopper = EarlyStopper(patience=5, min_delta=0.5)\n\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n            val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n            \n            training_dataset = EegEpochDataset(x,y)\n            training_dataloader = DataLoader(training_dataset, sampler=train_subsampler, batch_size = 32)\n            validation_dataloader = DataLoader(training_dataset, sampler=val_subsampler, batch_size = 32)\n\n            \n            for epoch in range(self.n_epoch):  # loop over the dataset multiple times\n\n                running_loss = 0.0\n                prediction_list = torch.empty(0).to(self.device)\n                true_list = torch.empty(0).to(self.device)\n                for i, data in enumerate(training_dataloader, 0):\n                    # get the inputs; data is a list of [inputs, labels]\n                    inputs, labels = data\n                    # torch.squeeze(inputs, dim=1)\n                    # print(inputs.shape)\n                    inputs, labels = inputs.to(self.device).float(), labels.to(self.device)\n\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n                    # forward + backward + optimize\n                    outputs = self.net.forward(inputs)\n                    loss = criterion(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n                    running_loss += loss.item()\n                    # training f1\n                    _, predicted = torch.max(outputs, 1)\n                    prediction_list = torch.cat([prediction_list, predicted])\n                    true_list = torch.cat([true_list, labels])\n\n                true_list = true_list.cpu().numpy()\n                prediction_list = prediction_list.cpu().numpy()\n\n                train_f1 = f1_score(true_list, prediction_list, average = 'macro')\n                train_error = 1-train_f1\n                self.training_error.append(train_error)\n\n                    # print statistics\n                print('epoch %d, %d samples, loss: %.3f' % (epoch + 1, (i+1)*training_dataloader.batch_size,running_loss / (i+1)), end = \", \")\n                print('training f1: %.3f' % (train_f1), end = ', ')\n                \n                running_loss = 0.0\n\n                #VALIDATION\n                with torch.no_grad():\n                    self.net.eval()\n                    validation_loss = 0.0\n                    prediction_list = torch.empty(0).to(self.device)\n                    true_list = torch.empty(0).to(self.device)\n                    for i, data in enumerate(validation_dataloader, 0):\n                        # get the inputs; data is a list of [inputs, labels]\n                        inputs, labels = data\n                        inputs, labels = inputs.to(self.device).float(), labels.to(self.device)\n\n                        # forward\n                        outputs = self.net.forward(inputs)\n                        loss = criterion(outputs, labels)\n                        \n                        validation_loss += loss.item()\n                        # evaluate f1 validation\n                        _, predicted = torch.max(outputs, 1)\n                        prediction_list = torch.cat([prediction_list, predicted])\n                        true_list = torch.cat([true_list, labels])\n                    \n                    true_list = true_list.cpu().numpy()\n                    prediction_list = prediction_list.cpu().numpy()\n\n                    validation_f1 = f1_score(true_list, prediction_list, average = 'macro')\n                    validation_error = 1-validation_f1\n                    self.val_error.append(validation_error)\n                    \n                    # print statistics\n                print('validation loss: %.3f' % (validation_loss / (i+1)), end=', ')\n                print('validation f1: %.3f' % (validation_f1))\n\n                if validation_error < (self.min_validation_error):\n                    self.min_validation_error = validation_error\n                    print('new minimal validation error')\n                    torch.save(self.net.state_dict(), 'opti_test')\n\n                if validation_loss < self.min_validation_loss:\n                    self.min_validation_loss = validation_loss\n                    torch.save(self.net.state_dict(), 'loss_test')\n\n                \n                if early_stopper.early_stop(validation_loss):    \n                    print('aie at epoch', epoch)         \n                    break\n                else:\n                    torch.save(self.net.state_dict(), 'my_net_opti')\n\n        \n            print('saved error so far: ', self.min_validation_error)\n            \n        self.net.load_state_dict(torch.load('opti_test'))\n        return(self)\n\n    def predict(self, x) :\n        check_is_fitted(self, ['X_', 'y_'])\n        x = check_array(x)\n        test_dataloader = DataLoader(EegEpochDataset_test(x))\n        with torch.no_grad():\n            prediction_list = torch.empty(0).to(self.device)\n            for data in test_dataloader:\n                inputs = data\n                inputs = inputs.to(self.device).float()\n                outputs = self.net(inputs)\n                _, predicted = torch.max(outputs, 1)\n                prediction_list = torch.cat([prediction_list, predicted])\n\n        return prediction_list.tolist()\n    \n    def predict_proba(self, x) :\n        check_is_fitted(self, ['X_', 'y_'])\n        x = check_array(x)\n        test_dataloader = DataLoader(EegEpochDataset_test(x))\n        with torch.no_grad():\n            prediction_list = torch.empty(0).to(self.device)\n            for data in test_dataloader:\n                inputs = data\n                inputs = inputs.to(self.device).float()\n                outputs = self.net(inputs)\n                prediction_list = torch.cat([prediction_list, outputs])\n\n        return prediction_list.tolist()\n\n\ndef weights(n) :\n    l = [rd.uniform(0,1) for i in range (n-1)]\n    l.sort()\n    l = [0] + l \n    l.append(1)\n    return([l[i] - l[i-1] for i in range (1,len(l))])\n\n\ndef scoring(y,pred) :\n    ConfusionMatrixDisplay.from_predictions(y,pred)\n    plt.show()\n    print({'balanced_accuracy': balanced_accuracy_score(y,pred),\n            'cohen_kappa': cohen_kappa_score(y,pred),\n            'macro_f1': f1_score(y,pred,average ='macro')})\n    \ndef by_category(y,pred) :\n    matrix = confusion_matrix(y,pred)\n    precision = [matrix[i][i]/sum(matrix[i]) for i in range (len(matrix))]\n    return precision","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"types = 'train', 'test'\nrecords_list = listdir(directory + \"training_records\")\n\nx_train, y_train = make_data_set(records_list)\n\nx, y = {},{}\nsplit = int(0.7*len(x_train))\n\nx['train'], y['train'] = x_train[:split], y_train[:split]\nx['test'], y['test'] = x_train[split:], y_train[split:]\n\ntest_records = listdir(directory + \"test_records\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models_spectral = 'random_forest', 'SVM', 'KNN', 'multi_layer_perceptron', 'decision_tree'\nmodels_funct = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10), \\\n        SVC(random_state = 42, max_iter=50, probability = True), \\\n        KNeighborsClassifier(n_neighbors=12, weights='distance'), \\\n        MLPClassifier(activation='tanh', max_iter=400), \\\n        DecisionTreeClassifier(max_depth=20)\navailable_cnns = SpectrogramCNN(), BestperformingConvNet(), SingleChannelConvNet()\n\npipelines = []\nweights_accuracy = []\n\n# FOR OTHER MODELS WITH SPECTRAL ANALYSIS AS INPUT \n\nfor m,model in enumerate(models_spectral):\n    pip = Pipeline([('spectral_analysis', spectral_and_acc()), ('scale', MinMaxScaler()), (model, models_funct[m])])\n    pip.fit(x['train'], y['train'])\n    pred = pip.predict(x['test'])\n    scoring(y['test'], pred)\n    weights_accuracy.append(by_category(y['test'], pred))\n    pipelines.append(pip)\n\n#FOR SINGLE CHANNEL SPECTRAL CNN \nfor i in range (n_EEG) :\n    pip = Pipeline([('spectro', EEG_unique(EEG_i =i)), ('cnn', CNN_code())])\n    pip.fit(x['train'], y['train'])\n    pred = pip.predict(x['test'])\n    scoring(y['test'], pred)\n    weights_accuracy.append(by_category(y['test'],pred))\n    pipelines.append(pip)\n\n\npip = Pipeline([('scale', MinMaxScaler()), ('cnn', CNN_code())])\npip.fit(x['train'], y['train'])\npred = pip.predict(x['test'])\nscoring(y['test'], pred)\nweights_accuracy.append(by_category(y['test'],pred))\npipelines.append(pip)\n\npredict = np.empty((len(pipelines), len(x['test']), 5))\nfor n in range(len(pipelines)):\n    pred = pipelines[n].predict_proba(x['test'])\n    predict[n] = np.array(pred)\n    w = np.array(weights_accuracy[n])\n    predict[n] = np.multiply(w, predict[n])\n\npredict = np.sum(predict, axis=0)\npredict = np.argmax(predict, axis=1)\nscoring(y['test'], predict)\n\nprint(weights_accuracy)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#del(pipelines[-1])\npip = Pipeline([('spectral_analysis', spectral_and_acc()), ('scale', MinMaxScaler()), ('cnn', CNN_code())])\npip.fit(x['train'], y['train'])\npred = pip.predict(x['test'])\nscoring(y['test'], pred)\nweights_accuracy.append(by_category(y['test'],pred))\npipelines.append(pip)\n\npredict = np.empty((len(pipelines), len(x['test']), 5))\nfor n in range(len(pipelines)):\n    pred = pipelines[n].predict_proba(x['test'])\n    predict[n] = np.array(pred)\n    w = np.array(weights_accuracy[n])\n    predict[n] = np.multiply(w, predict[n])\n\npredict = np.sum(predict, axis=0)\npredict = np.argmax(predict, axis=1)\nscoring(y['test'], predict)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nX_size = 0\npred_size = 0\nfor test in test_records:\n    X_test = make_data_set_test(test)\n    size, _ = X_test.shape\n    X_size += size\n    predict = np.empty((len(pipelines), len(X_test), 5))\n    for n in range(len(pipelines)):\n        pred = pipelines[n].predict_proba(X_test)\n        predict[n] = np.array(pred)\n        w = np.array(weights_accuracy[n])\n        predict[n] = np.multiply(w, predict[n])\n    predict = np.sum(predict, axis=0)\n    predict = np.argmax(predict, axis=1)\n    pred_size += len(predict)\n    record_number = int(test[-5])\n    for i, pred in enumerate(predict):\n        predictions.append({\"identifier\":record_number * 10000 + i,'target':int(pred)})\n\npredictions = pd.DataFrame(predictions)\nprint(predictions)\npredictions.to_csv('submission.csv',index = None)\n\nif len(predictions) != 2646:\n    print('WARNING: incorrect submission length! (', len(predictions), ' instead of 2646)')\nelse:\n    print('GOOD TO GO! CORRECT SIZE')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle competitions submit -c dreem-automated-sleep-staging -f submission.csv -m \"Message\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}