{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"vscode":{"interpreter":{"hash":"0b273c37ffabc846887c9c57ad259dadbe4b7d60fa7973c931ec1b13f33c760d"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Let the challenge begin\n\n**Notes on data** \n\n- 5 EEG derivations sampled at 250Hz\n- 3 Accelerometers derivations sampled at 50Hz\n- Sleep epoch = 30 sec\n- hypnogram = succession of the sleep stages (0...5)\n\n**General info sleep**\n- Sleep stages = (N1, N2) = light sleep, N3 = deep sleep, REM\n- Low frequency power: N3 > N2 > N1-REM-Wake\n\n**Wake**\n- During Wake epoch alpha waves are clearly visible on the F-O derivation\n- Movement occured mainly during wake periods, noisy signals during movement\n- Alpha wave frequency ranges between 8 and 13 hertz = wake, relaxed\n**N1**\n- Theta waves freq betw 4 and 8 Hz = N1, N2\n\n**N2**\n- On N2 epoch, power in the spindle range is much higher on frontal-frontal channels\n- Theta waves freq betw 4 and 8 Hz = N1, N2\n- During N2, sleep spindles (fast rythm between 12-14Hz which last between 0.5 up to 2 seconds) are more visible on the Frontal-frontal derivation\n\n**N3**\n- On N3 epoch, we can see more power in the low frequencies\n- Delta waves freq betw 1 and 4 Hz = N3\n\n**REM**\n- REM sleep distinguishable with steady EEG and eyes movement which can be seen when looking at Frontal-occipital vs frontal-frontal derivation.\n- The EEG power increases in the low-frequency band when the sleep stage change from REM to NREM sleep stages\n- REM epoch have more steady EEG\n\n**Formulas**\n- Spectrogram are the time-frequency matrix z = P(t, f)\n- Spectrum correspond to the curves y = P(frequency)\n- Average Spectrum can therefore be computed as the mean of spectromgram over a specified period \n\n**Links**\nhttps://opentext.wsu.edu/psych105/chapter/stages-of-sleep/\nhttps://www.sleepfoundation.org/how-sleep-works/alpha-waves-and-sleep\nhttps://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/Challenge%20Data%20Dreem-1.pdf\nhttps://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/entropy-18-00272.pdf\n\n","metadata":{}},{"cell_type":"code","source":"cd \"/kaggle/input/dreem-automated-sleep-staging/\"","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:37:51.524251Z","iopub.execute_input":"2023-01-02T13:37:51.524841Z","iopub.status.idle":"2023-01-02T13:37:51.532868Z","shell.execute_reply.started":"2023-01-02T13:37:51.524806Z","shell.execute_reply":"2023-01-02T13:37:51.531287Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"/kaggle/input/dreem-automated-sleep-staging\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install lspopt\n!pip install skorch\n!pip install cuda-python\n\ndirectory = \"/kaggle/input/dreem-automated-sleep-staging/\"\nout_directory = '/kaggle/working/'\n\n#directory = './'\n#out_directory = './'","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:37:51.535403Z","iopub.execute_input":"2023-01-02T13:37:51.535892Z","iopub.status.idle":"2023-01-02T13:38:19.092944Z","shell.execute_reply.started":"2023-01-02T13:37:51.535858Z","shell.execute_reply":"2023-01-02T13:38:19.091692Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Requirement already satisfied: lspopt in /opt/conda/lib/python3.7/site-packages (1.2.0)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from lspopt) (1.7.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from lspopt) (1.15.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from lspopt) (1.21.6)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: skorch in /opt/conda/lib/python3.7/site-packages (0.12.1)\nRequirement already satisfied: scikit-learn>=0.22.0 in /opt/conda/lib/python3.7/site-packages (from skorch) (1.0.2)\nRequirement already satisfied: numpy>=1.13.3 in /opt/conda/lib/python3.7/site-packages (from skorch) (1.21.6)\nRequirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from skorch) (0.9.0)\nRequirement already satisfied: scipy>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from skorch) (1.7.3)\nRequirement already satisfied: tqdm>=4.14.0 in /opt/conda/lib/python3.7/site-packages (from skorch) (4.64.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.0->skorch) (3.1.0)\nRequirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn>=0.22.0->skorch) (1.0.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: cuda-python in /opt/conda/lib/python3.7/site-packages (12.0.0)\nRequirement already satisfied: cython in /opt/conda/lib/python3.7/site-packages (from cuda-python) (0.29.32)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"#directory = '/kaggle/input/files-ml/'\n#out_directory = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:34:31.486282Z","iopub.execute_input":"2023-01-02T13:34:31.487067Z","iopub.status.idle":"2023-01-02T13:34:31.498530Z","shell.execute_reply.started":"2023-01-02T13:34:31.487024Z","shell.execute_reply":"2023-01-02T13:34:31.497497Z"}}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib as mpl\nfrom matplotlib.colors import Normalize\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom os import listdir\nfrom random import randint\nimport random as rd\nimport os \n\nfrom lspopt import spectrogram_lspopt\nfrom scipy.signal import spectrogram\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\nfrom sklearn.utils.validation import check_X_y, check_array, check_is_fitted\nfrom sklearn.utils.multiclass import unique_labels\n\nfrom sklearn.metrics import plot_confusion_matrix, f1_score\nfrom sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n\nimport torch.optim as optim\nfrom skorch import NeuralNetClassifier\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torch\nimport torchaudio.transforms as T\n\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.model_selection import ParameterGrid, GridSearchCV\n\nimport joblib","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:19.095287Z","iopub.execute_input":"2023-01-02T13:38:19.095715Z","iopub.status.idle":"2023-01-02T13:38:19.109100Z","shell.execute_reply.started":"2023-01-02T13:38:19.095672Z","shell.execute_reply":"2023-01-02T13:38:19.107905Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"frequency_bands = {\n        \"delta\": [0.5, 4],\n        \"theta\": [4, 8],\n        \"alpha\": [8, 12],\n       \"sigma\": [12, 16],\n       \"beta\": [16, 30]\n    }\n\nEEG_FS = 250\nACC_FS = 50 \nepoch_s = 30\nn_EEG = 5\nn_ACC = 3\n\nhypnograms = pd.read_csv(directory + 'targets_train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:19.112931Z","iopub.execute_input":"2023-01-02T13:38:19.113703Z","iopub.status.idle":"2023-01-02T13:38:19.150870Z","shell.execute_reply.started":"2023-01-02T13:38:19.113664Z","shell.execute_reply":"2023-01-02T13:38:19.149778Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"#plt.imshow(plt.imread(directory + 'corr_sleep_stages.png'))","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:19.152020Z","iopub.execute_input":"2023-01-02T13:38:19.152368Z","iopub.status.idle":"2023-01-02T13:38:19.158890Z","shell.execute_reply.started":"2023-01-02T13:38:19.152314Z","shell.execute_reply":"2023-01-02T13:38:19.157776Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Functions ","metadata":{}},{"cell_type":"code","source":"def get_average_spectrum_for_epochs(eeg,epochs):\n    \"\"\"\n    Return the average power in each of the fourier bin for several epochs.\n    \"\"\"\n    psds = []\n    for epoch in epochs:\n        idx_start,idx_end = 250 * 30 * epoch,250 * 30 * (epoch + 1)\n        freqs,t,psd = spectrogram_lspopt(np.clip(eeg[idx_start:idx_end],-150,150),250,nperseg = 1000)\n        psds += [np.mean(psd ** 2,1)]\n    return freqs,np.array(psds).mean(0)\n\n\n## Function to plot N sleep epochs for a specific stage\n\ndef random_sleep_epoch(N, sleep_stage, hypnogram) :\n    k = 0\n    a = randint(0,len(hypnogram))\n    epochs = []\n    while k < N:\n        if hypnogram[a] == sleep_stage :\n            epochs.append(a)\n            k += 1\n            a = randint(0,len(hypnogram))\n        else :\n            a = randint(0,len(hypnogram))\n    eeg_ff = np.load('sample/sample/f8_f7.npy')\n    for epoch in epochs : \n        t0 = epoch*epoch_s*EEG_FS\n        eeg_short = eeg_ff[t0:t0+(epoch_s*EEG_FS)]\n        plt.figure(figsize=(25, 8))\n        plt.plot(eeg_short)\n        plt.ylim([-200, 200])\n        plt.xlim(0,len(eeg_short))\n        plt.show()\n\ndef plot_spectrogram(\n    data,\n    sf,\n    hypno=None,\n    win_sec=30,\n    fmin=0.5,\n    fmax=25,\n    trimperc=2.5,\n    cmap=\"RdBu_r\",\n    vmin=None,\n    vmax=None,\n):\n    # Safety checks\n    assert isinstance(data, np.ndarray), \"Data must be a 1D NumPy array.\"\n    assert isinstance(sf, (int, float)), \"sf must be int or float.\"\n    assert data.ndim == 1, \"Data must be a 1D (single-channel) NumPy array.\"\n    assert isinstance(win_sec, (int, float)), \"win_sec must be int or float.\"\n    assert isinstance(fmin, (int, float)), \"fmin must be int or float.\"\n    assert isinstance(fmax, (int, float)), \"fmax must be int or float.\"\n    assert fmin < fmax, \"fmin must be strictly inferior to fmax.\"\n    assert fmax < sf / 2, \"fmax must be less than Nyquist (sf / 2).\"\n    assert isinstance(vmin, (int, float, type(None))), \"vmin must be int, float, or None.\"\n    assert isinstance(vmax, (int, float, type(None))), \"vmax must be int, float, or None.\"\n    if vmin is not None:\n        assert isinstance(vmax, (int, float)), \"vmax must be int or float if vmin is provided\"\n    if vmax is not None:\n        assert isinstance(vmin, (int, float)), \"vmin must be int or float if vmax is provided\"\n\n    # Calculate multi-taper spectrogram\n    nperseg = int(win_sec * sf)\n    assert data.size > 2 * nperseg, \"Data length must be at least 2 * win_sec.\"\n    f, t, Sxx = spectrogram_lspopt(data, sf, nperseg=nperseg, noverlap=0)\n    Sxx = 10 * np.log10(Sxx)  # Convert uV^2 / Hz --> dB / Hz\n\n    # Select only relevant frequencies (up to 30 Hz)\n    good_freqs = np.logical_and(f >= fmin, f <= fmax)\n    Sxx = Sxx[good_freqs, :]\n    f = f[good_freqs]\n    t /= 3600  # Convert t to hours\n\n    # Normalization\n    if vmin is None:\n        vmin, vmax = np.percentile(Sxx, [0 + trimperc, 100 - trimperc])\n        norm = Normalize(vmin=vmin, vmax=vmax)\n    else:\n        norm = Normalize(vmin=vmin, vmax=vmax)\n\n    if hypno is None:\n        fig, ax = plt.subplots(nrows=1, figsize=(3, 1))\n        im = ax.pcolormesh(t, f, Sxx, norm=norm, cmap=cmap, antialiased=True, shading=\"auto\")\n        ax.set_xlim(0, t.max())\n        plt.setp([ax.get_xticklines() + ax.get_yticklines() + ax.get_xgridlines() + ax.get_ygridlines()],antialiased=False)\n        mpl.rcParams['text.antialiased']=False\n        fig.tight_layout(pad=0)\n        return fig\n\ndef split_train_test(records, split=0.5) : \n    rd.seed(1234)\n    rd.shuffle(records)\n    k = int(len(records)*split)\n    return records[:k],records[k:]\n\n\ndef spectral_power(data,n,fs) :\n    for i in range (n) :\n        sfreqs,t,psd = spectrogram(data[:,i,:], fs, nperseg = 1000,noverlap = 750)\n        psd = np.mean(np.abs(psd),-1)\n        l = []\n        for name, freqband in frequency_bands.items():\n            spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n            spec_power = np.sum(spec_power, 1)\n            l.append(spec_power / np.sum(psd,1))\n        matrice = np.array(l) \n        matrice = np.vstack((matrice, np.array([np.mean(data[k,i,:]) for k in range (len(data))]).T))\n        matrice = np.vstack((matrice, np.array([np.std(data[k,i,:]) for k in range (len(data))]).T))\n        if i == 0:\n            complete_array = matrice \n        else :\n            complete_array = np.vstack((complete_array,matrice))\n    return(complete_array.T)\n\n\ndef only_stats(data,n) : \n    for i in range (n) :\n        matrice = np.array([np.mean(data[k,i,:]) for k in range (len(data))]).T\n        matrice = np.vstack((matrice, np.array([np.std(data[k,i,:]) for k in range (len(data))]).T))\n        if i == 0:\n            complete_array = matrice \n        else :\n            complete_array = np.vstack((complete_array,matrice))\n    return(complete_array.T)\n\n\ndef correlations(record, data, n) :\n    corr = [0]*n\n    for i in range (n) :\n        corr[i] = [0]*n\n        for j in range (n) :\n            corr[i][j] = np.corrcoef(data[record,i,:], data[record,j,:])[0][1]\n        print(i+1,corr[i]) \n    return corr  \n\ndef whitening(X):  \n    cov = np.cov(X)\n    d, E = np.linalg.eigh(cov)\n    D = np.diag(d)\n    D_inv = np.sqrt(np.linalg.inv(D))\n    X_whiten = np.dot(E, np.dot(D_inv, np.dot(E.T, X)))\n    return X_whiten\n\ndef normalize_data(eeg_array):\n    \"\"\"normalize signal between 0 and 1\"\"\"\n\n    normalized_array = np.clip(eeg_array, -250, 250)\n    normalized_array = normalized_array / 250\n\n    return normalized_array\n\n\nclass EegEpochDataset(Dataset):\n    \"\"\"EEG Epochs dataset.\"\"\"\n\n    def __init__(self, x_data, y_data, transform=None):\n        \"\"\"\n        Args:\n            x_data (numpy array): Numpy array of input data.\n            y_data (list of numpy array): Sleep Stages\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.y_data = y_data\n        self.x_data = x_data\n        self.transform = transform\n\n        self.x_data = normalize_data(x_data)\n\n    def __len__(self):\n        return len(self.y_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        signal = np.expand_dims(self.x_data[idx], axis=0)\n        stage = self.y_data[idx]\n\n        if self.transform:\n            signal = self.transform(signal)\n\n        return signal, stage\n    \nclass EegEpochDataset_test(Dataset):\n    \"\"\"EEG Epochs dataset.\"\"\"\n\n    def __init__(self, x_data, transform=None):\n        \"\"\"\n        Args:\n            x_data (numpy array): Numpy array of input data.\n            y_data (list of numpy array): Sleep Stages\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.x_data = x_data\n        self.transform = transform\n\n        self.x_data = normalize_data(x_data)\n        \n    def __len__(self):\n        return len(self.x_data)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        signal = np.expand_dims(self.x_data[idx], axis=0)\n\n        if self.transform:\n            signal = self.transform(signal)\n\n        return signal\n\n\nclass EarlyStopper:\n    def __init__(self, patience=1, min_delta=0):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.counter = 0\n        self.min_validation_loss = np.inf\n\n    def early_stop(self, validation_loss):\n        if validation_loss < self.min_validation_loss:\n            self.min_validation_loss = validation_loss\n            self.counter = 0\n        elif validation_loss > (self.min_validation_loss + self.min_delta):\n            self.counter += 1\n            if self.counter >= self.patience:\n                return True\n        return False\n\ndef weights_init(m):\n    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        torch.nn.init.xavier_uniform_(m.weight.data)\n\nclass SingleChannelConvNet(nn.Module):\n\n    def __init__(self):\n        super(SingleChannelConvNet, self).__init__()\n        self.conv_a = nn.Conv1d(1, 8, 5, stride=5)\n        self.batchnorm_a = nn.BatchNorm1d(8, eps=0.001, momentum=0.99)\n        self.conv_b = nn.Conv1d(8, 32, 5, stride=3)\n        self.batchnorm_b = nn.BatchNorm1d(32, eps=0.001, momentum=0.99)\n        self.conv_c = nn.Conv1d(32, 64, 3, stride=3)\n        self.batchnorm_c= nn.BatchNorm1d(64, eps=0.001, momentum=0.99)\n        self.conv_d = nn.Conv1d(64, 128, 3, stride=2)\n        self.batchnorm_d= nn.BatchNorm1d(128, eps=0.001, momentum=0.99)\n        self.conv_e = nn.Conv1d(128, 256, 3, stride=1)\n        self.batchnorm_e= nn.BatchNorm1d(256, eps=0.001, momentum=0.99)\n       \n        self.max_pool_a = nn.MaxPool1d(5, stride=1)\n        self.max_pool_prime = nn.MaxPool1d(3, stride=1)\n        self.dropout = nn.Dropout(p=0.4)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(118784, 256)\n        self.fc2 = nn.Linear(256, 5)\n        self.fc_lin = nn.Linear(256,5)\n\n    def forward(self, x):\n        x = self.relu(self.batchnorm_a(self.conv_a(x)))\n        # x = self.max_pool_a(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_b(self.conv_b(x)))\n        # x = self.max_pool_prime(x)\n        x = self.dropout(x)\n        x = self.relu(self.batchnorm_c(self.conv_c(x)))\n        # x = self.max_pool_prime(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_d(self.conv_d(x)))\n        # x = self.max_pool_prime(x)\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_e(self.conv_e(x)))\n        # x = self.max_pool_prime(x)\n        x = self.dropout(x)\n        x = x.max(-1)[0]\n        # x = self.softmax(x)\n        # x = torch.flatten(x,1)\n        # x = self.fc1(x)\n        # x = self.fc2(x)\n        x = self.fc_lin(x)\n        # x = F.log_softmax(x, dim=1) #at the moment the softmax is bad\n        # x = self.avg_pool(x)\n        # x = self.max_pool(x)\n\n        return x\n\nclass BestperformingConvNet(nn.Module):\n\n    def __init__(self):\n        super(BestperformingConvNet, self).__init__()\n        self.conv_a = nn.Conv1d(1, 8, 5, stride=5)\n        self.batchnorm_a = nn.BatchNorm1d(8, eps=0.001, momentum=0.99)\n        self.conv_b = nn.Conv1d(8, 32, 5, stride=3)\n        self.batchnorm_b = nn.BatchNorm1d(32, eps=0.001, momentum=0.99)\n        self.conv_c = nn.Conv1d(32, 64, 3, stride=3)\n        self.batchnorm_c= nn.BatchNorm1d(64, eps=0.001, momentum=0.99)\n        self.conv_d = nn.Conv1d(64, 128, 3, stride=2)\n        self.batchnorm_d= nn.BatchNorm1d(128, eps=0.001, momentum=0.99)\n        self.conv_e = nn.Conv1d(128, 256, 3, stride=1)\n        self.batchnorm_e= nn.BatchNorm1d(256, eps=0.001, momentum=0.99)\n\n        self.max_pool_a = nn.MaxPool1d(5, stride=1)\n        self.max_pool_prime = nn.MaxPool1d(3, stride=1)\n        self.dropout = nn.Dropout(p=0.2)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(117760, 128)\n        self.fc2 = nn.Linear(128, 5)\n        self.fc_lin = nn.Linear(256,5)\n\n    def forward(self, x):\n\n        x = self.relu(self.batchnorm_a(self.conv_a(x)))\n        x = self.dropout(x)\n        x = self.relu(self.batchnorm_b(self.conv_b(x)))\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_c(self.conv_c(x)))\n        # x = self.dropout(x)\n        x = self.relu(self.batchnorm_d(self.conv_d(x)))\n        x = self.dropout(x)\n        x = self.relu(self.batchnorm_e(self.conv_e(x)))\n        x = x.max(-1)[0]\n        x = self.fc_lin(x)\n        # x = F.log_softmax(x, dim=1) #at the moment the softmax is bad\n\n        return x\n\n\nclass SpectrogramCNN(nn.Module):\n\n    def __init__(self):\n        super(SpectrogramCNN, self).__init__()\n        self.conv_a = nn.Conv2d(1, 8, 25, stride=3)\n        self.conv_b = nn.Conv2d(8, 16, 15, stride=2)\n        self.conv_c = nn.Conv2d(16, 64, 5, stride=1)\n        self.conv_d = nn.Conv2d(64, 128, 2, stride=1)\n        self.conv_e = nn.Conv2d(128, 256, 2, stride=1)\n       \n        self.MP_prime= nn.MaxPool2d(5, stride=1)\n        self.MP_ultra = nn.MaxPool2d(5, stride = 5)\n        \n        self.dropout1 = nn.Dropout(p=0.2)\n        self.dropout2 = nn.Dropout(p=0.2)\n        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n        self.relu = nn.ReLU()\n\n        self.fc1 = nn.Linear(19200, 128)\n        self.fc2 = nn.Linear(128, 5)\n\n    def forward(self, x):\n        transf = T.Spectrogram(n_fft=528, win_length=None, hop_length=int(EEG_FS * 150e-3), center=True, pad_mode=\"reflect\",power=2.0)\n        x = transf(x)\n        x = self.conv_a(x)\n        x = self.relu(x)\n        # x = self.MP_prime(x)\n        x = self.dropout1(x)\n        x = self.conv_b(x)\n        x = self.relu(x)\n        x = self.dropout2(x)\n        x = self.MP_prime(x)\n        x = self.relu(self.conv_c(x))\n        x = self.dropout1(x)\n        x = self.relu(self.conv_d(x))\n        x = self.MP_ultra(x)\n        # x = x.max(-1)[0]\n\n        x = torch.flatten(x, 1)\n        x = self.relu(self.fc1(x))\n        x = self.fc2(x)\n\n        return x\n\ndef make_data_set(record):\n    if type(record) == list:\n        data = np.load(f'{directory}training_records/{record[0]}')\n        record_number = int(record[0][-5])\n        hypnogram = list(hypnograms[hypnograms['record'] == record_number]['target'])\n        record = [record[0]]\n        for r in range(1,len(record)):\n            record_number = int(record[r][-5])\n            h = list(hypnograms[hypnograms['record'] == record_number]['target'])\n            hypnogram.extend(h)\n            d = np.load(f'{directory}training_records/{record[r]}')\n            data = np.vstack((data,d))\n            record.append(record[r])\n    else:\n        data = np.load(f'{directory}training_records/{record}')\n        record_number = int(record[-5])\n        hypnogram = list(hypnograms[hypnograms['record'] == record_number]['target'])\n        record = [record]\n    return(data,hypnogram)\n\ndef make_data_set_test(record):\n    if type(record) == list:\n        data = np.load(f'{directory}test_records/{record[0]}')\n        record_number = int(record[0][-5])\n        record = [record[0]]\n        for r in range(1,len(record)):\n            record_number = int(record[r][-5])\n            d = np.load(f'{directory}test_records/{record[r]}')\n            data = np.vstack((data,d))\n            record.append(record[r])\n    else:\n        data = np.load(f'{directory}test_records/{record}')\n        record = [record]\n    return(data)\n\ndef EEG_ACC(data) :\n    EEG = data[:,1:EEG_FS * epoch_s * n_EEG + 1]\n    EEG = EEG.reshape(len(data), n_EEG, EEG_FS * epoch_s)\n    ACC = data[:,EEG_FS * epoch_s * n_EEG + 1:]\n    ACC = ACC.reshape(len(data), n_ACC, ACC_FS * epoch_s)\n    return EEG, ACC\n\ndef EEG_spectral(EEG) :\n    EEG_spectral_power = spectral_power(EEG,n_EEG,EEG_FS)\n    return EEG_spectral_power\n\ndef ACC_statistics(ACC):\n    return only_stats(ACC,n_ACC)\n\ndef image_spectrale(EEG, eeg_i) :\n    EEG_i = EEG[:,eeg_i,:]\n    images = [0]*len(EEG_i)\n    for i,eeg in enumerate(EEG_i) :\n        fig = plot_spectrogram(np.clip(eeg,-200,200), 250, cmap='Spectral_r', win_sec = 5,trimperc = 2)\n        fig.canvas.draw ()\n        mat = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n        mat = mat.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n        images[i] = mat\n        plt.close()\n    return np.array(images)\n    \nclass spectral_img(BaseEstimator, TransformerMixin) :\n    def __init__(self,EEG_i = 0) :\n        self.EEG_i = EEG_i \n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG,_ = EEG_ACC(x)\n        self.img = image_spectrale(EEG, self.EEG_i)\n        n =self.img.shape[1]*self.img.shape[2]*self.img.shape[3]\n        return self.img.reshape(self.img.shape[0], n).astype('float32')\n\nclass EEG_unique(BaseEstimator, TransformerMixin) : \n    def __init__(self,EEG_i = 0) :\n        self.EEG_i = EEG_i \n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG,_ = EEG_ACC(x)\n        return EEG[:,self.EEG_i,:]\n\nclass spectral_and_acc(BaseEstimator, TransformerMixin) :\n    def __init__(self) :\n        return None\n\n    def fit(self, x, y) :\n        return self\n\n    def transform(self, x) :\n        EEG, ACC = EEG_ACC(x)\n        self.EEG_spectral_power = EEG_spectral(EEG)\n        self.ACC_stats = ACC_statistics(ACC)\n        EEG_and_ACC_spectral_power = np.vstack(((self.EEG_spectral_power).T, (self.ACC_stats).T))\n        return EEG_and_ACC_spectral_power.T\n\n\nclass CNN_code(ClassifierMixin, BaseEstimator) :\n\n    def __init__(self):\n        self.net = SingleChannelConvNet()\n        # device: use GPU if available\n        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n        self.net = self.net.to(self.device) # model into GP\n        self.n_epoch =100\n        self.min_validation_loss = np.inf\n        self.min_validation_error = np.inf\n        self.n_splits = 10\n        self.learning_rate = 1e-3\n        \n    \n    def fit(self, x, y) :\n        criterion = nn.CrossEntropyLoss()\n        optimizer = optim.Adam(self.net.parameters())\n        x, y = check_X_y(x, y)\n        self.classes_ = unique_labels(y)\n        self.X_ = x\n        self.y_ = y\n        self.net.train()\n        print('training...')\n\n        #KFold setting\n        KCV = KFold(n_splits=self.n_splits) #shuffle=True, random_state=15011999 \n        self.val_error = []\n        self.training_error = []\n        for fold, (train_index, val_index) in enumerate(KCV.split(x)):\n            print('------------fold no---------{}----------------------'.format(fold+1))\n            # weight resetting: avoid weight leakage\n            self.net.apply(weights_init)\n\n            early_stopper = EarlyStopper(patience=5, min_delta=0.5)\n\n            train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n            val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n            \n            training_dataset = EegEpochDataset(x,y)\n            training_dataloader = DataLoader(training_dataset, sampler=train_subsampler, batch_size = 32)\n            validation_dataloader = DataLoader(training_dataset, sampler=val_subsampler, batch_size = 32)\n\n            \n            for epoch in range(self.n_epoch):  # loop over the dataset multiple times\n\n                running_loss = 0.0\n                prediction_list = torch.empty(0).to(self.device)\n                true_list = torch.empty(0).to(self.device)\n                for i, data in enumerate(training_dataloader, 0):\n                    # get the inputs; data is a list of [inputs, labels]\n                    inputs, labels = data\n                    # torch.squeeze(inputs, dim=1)\n                    # print(inputs.shape)\n                    inputs, labels = inputs.to(self.device).float(), labels.to(self.device)\n\n                    # zero the parameter gradients\n                    optimizer.zero_grad()\n                    # forward + backward + optimize\n                    outputs = self.net.forward(inputs)\n                    loss = criterion(outputs, labels)\n                    loss.backward()\n                    optimizer.step()\n                    running_loss += loss.item()\n                    # training f1\n                    _, predicted = torch.max(outputs, 1)\n                    prediction_list = torch.cat([prediction_list, predicted])\n                    true_list = torch.cat([true_list, labels])\n\n                true_list = true_list.cpu().numpy()\n                prediction_list = prediction_list.cpu().numpy()\n\n                train_f1 = f1_score(true_list, prediction_list, average = 'macro')\n                train_error = 1-train_f1\n                self.training_error.append(train_error)\n\n                    # print statistics\n                print('epoch %d, %d samples, loss: %.3f' % (epoch + 1, (i+1)*training_dataloader.batch_size,running_loss / (i+1)), end = \", \")\n                print('training f1: %.3f' % (train_f1), end = ', ')\n                \n                running_loss = 0.0\n\n                #VALIDATION\n                with torch.no_grad():\n                    self.net.eval()\n                    validation_loss = 0.0\n                    prediction_list = torch.empty(0).to(self.device)\n                    true_list = torch.empty(0).to(self.device)\n                    for i, data in enumerate(validation_dataloader, 0):\n                        # get the inputs; data is a list of [inputs, labels]\n                        inputs, labels = data\n                        inputs, labels = inputs.to(self.device).float(), labels.to(self.device)\n\n                        # forward\n                        outputs = self.net.forward(inputs)\n                        loss = criterion(outputs, labels)\n                        \n                        validation_loss += loss.item()\n                        # evaluate f1 validation\n                        _, predicted = torch.max(outputs, 1)\n                        prediction_list = torch.cat([prediction_list, predicted])\n                        true_list = torch.cat([true_list, labels])\n                    \n                    true_list = true_list.cpu().numpy()\n                    prediction_list = prediction_list.cpu().numpy()\n\n                    validation_f1 = f1_score(true_list, prediction_list, average = 'macro')\n                    validation_error = 1-validation_f1\n                    self.val_error.append(validation_error)\n                    \n                    # print statistics\n                print('validation loss: %.3f' % (validation_loss / (i+1)), end=', ')\n                print('validation f1: %.3f' % (validation_f1))\n\n                if validation_error < (self.min_validation_error):\n                    self.min_validation_error = validation_error\n                    print('new minimal validation error')\n                    torch.save(self.net.state_dict(), 'opti_test')\n\n                if validation_loss < self.min_validation_loss:\n                    self.min_validation_loss = validation_loss\n                    torch.save(self.net.state_dict(), 'loss_test')\n\n                \n                if early_stopper.early_stop(validation_loss):    \n                    print('aie at epoch', epoch)         \n                    break\n                else:\n                    torch.save(self.net.state_dict(), 'my_net_opti')\n\n            \n            print('saved error so far: ', self.min_validation_error)\n\n        return(self)\n\n    def predict(self, x) :\n        check_is_fitted(self, ['X_', 'y_'])\n        x = check_array(x)\n        test_dataloader = DataLoader(EegEpochDataset_test(x))\n        with torch.no_grad():\n            prediction_list = torch.empty(0).to(self.device)\n            for data in test_dataloader:\n                inputs = data\n                inputs = inputs.to(self.device).float()\n                outputs = self.net(inputs)\n                _, predicted = torch.max(outputs, 1)\n                prediction_list = torch.cat([prediction_list, predicted])\n\n        return prediction_list.tolist()\n    \n    def predict_proba(self, x) :\n        check_is_fitted(self, ['X_', 'y_'])\n        x = check_array(x)\n        test_dataloader = DataLoader(EegEpochDataset_test(x))\n        with torch.no_grad():\n            prediction_list = torch.empty(0).to(self.device)\n            for data in test_dataloader:\n                inputs = data\n                inputs = inputs.to(self.device).float()\n                outputs = self.net(inputs)\n                prediction_list = torch.cat([prediction_list, outputs])\n\n        return prediction_list.tolist()\n\n\ndef weights(n) :\n    l = [rd.uniform(0,1) for i in range (n-1)]\n    l.sort()\n    l = [0] + l \n    l.append(1)\n    return([l[i] - l[i-1] for i in range (1,len(l))])\n\n\ndef scoring(y,pred) :\n    ConfusionMatrixDisplay.from_predictions(y,pred)\n    plt.show()\n    print({'balanced_accuracy': balanced_accuracy_score(y,pred),\n            'cohen_kappa': cohen_kappa_score(y,pred),\n            'macro_f1': f1_score(y,pred,average ='macro')})\n    ","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:19.161238Z","iopub.execute_input":"2023-01-02T13:38:19.162172Z","iopub.status.idle":"2023-01-02T13:38:19.276338Z","shell.execute_reply.started":"2023-01-02T13:38:19.162091Z","shell.execute_reply":"2023-01-02T13:38:19.275440Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Useful preprocessing (using fit_transform(x))\n- Normalizer()\n- MinMaxScaler()\n- TruncatedSVD()\n- PCA(random_state=42, svd_solver=solver, n_components=n)  (solver = 'arpack' or 'auto')\n- FastICA(random_state=42, n_components=n, whiten=False) if whiten False use whitening function","metadata":{}},{"cell_type":"code","source":"types = 'train', 'test'\nrecords_list = listdir(directory + \"training_records\")\nrecords = {}\nrecords[types[0]], records[types[1]] = split_train_test(records_list, split=0.2)\nx = {}\ny = {}\nfor t in types:\n    x[t], y[t]= make_data_set(records[t])\n\ntest_records = listdir(directory + \"test_records\")","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:19.277714Z","iopub.execute_input":"2023-01-02T13:38:19.278036Z","iopub.status.idle":"2023-01-02T13:38:22.585140Z","shell.execute_reply.started":"2023-01-02T13:38:19.278003Z","shell.execute_reply":"2023-01-02T13:38:22.584053Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"models_spectral = 'random_forest', 'SVM', 'KNN', 'multi_layer_perceptron', 'decision_tree'\nmodels_funct = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10), \\\n        SVC(random_state = 42, max_iter=150, probability = True), \\\n        KNeighborsClassifier(n_neighbors=10, weights='distance'), \\\n        MLPClassifier(activation='tanh', max_iter=300), \\\n        DecisionTreeClassifier(max_depth=15)\navailable_cnns = SpectrogramCNN(), BestperformingConvNet(), SingleChannelConvNet()\n\npipelines = []\n\n# FOR OTHER MODELS WITH SPECTRAL ANALYSIS AS INPUT \n\nfor m,model in enumerate(models_spectral):\n    pip = Pipeline([('spectral_analysis', spectral_and_acc()), ('scale', MinMaxScaler()), (model, models_funct[m])])\n    pipelines.append((f'pipe_{model}', pip))\n\n# FOR SINGLE CHANNEL SPECTRAL CNN \nfor i in range (n_EEG) :\n    pip = Pipeline([('spectro', EEG_unique(EEG_i =i)), ('cnn', CNN_code())])\n    #pip.fit(x['train'], y['train'])\n    pipelines.append((f'pipe_cnn_{i}', pip))\n\nl = len(pipelines)\nvc = VotingClassifier(estimators=pipelines, voting='soft',n_jobs=-1, weights = weights(l))\nn=3\n\nweights_grid = [weights(l) for _ in range (n)]\nfor i in range (n) :\n    weights_grid.append(weights(l))\nweights_grid = {'weights':weights_grid}\ngrid_search = GridSearchCV(param_grid=weights_grid,estimator = vc, cv=n, n_jobs=-1, verbose=10, scoring=\"f1_macro\")\ngrid_search.fit(x['train'], y['train'])\nprint(grid_search.best_score_)\n\n# for i in range (n) :\n#     a = int((i)*x['train'].shape[0]/n) \n#     b = int((i+1)*x['train'].shape[0]/n) -1\n#     vc.set_params(weights = weights(l))\n#     vc.fit(x['train'][a:b], y['train'][a:b])\n#     pred = vc.predict(x['test'])\n#     scoring(y['test'], pred)\n#     filename = f\"{out_directory}votingclass_{i}.joblib.pkl\"\n#     joblib.dump(vc, filename, compress=9)\n    \n","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:38:22.586760Z","iopub.execute_input":"2023-01-02T13:38:22.587213Z","iopub.status.idle":"2023-01-02T13:44:05.079044Z","shell.execute_reply.started":"2023-01-02T13:38:22.587173Z","shell.execute_reply":"2023-01-02T13:44:05.077197Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 6 candidates, totalling 18 fits\n[CV 1/3; 1/6] START weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294]\ntraining...\n------------fold no---------1----------------------\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 4.058, training f1: 0.257, epoch 1, 512 samples, loss: 3.282, training f1: 0.297, validation loss: 1.472, validation f1: 0.296\nnew minimal validation error\nvalidation loss: 1.172, validation f1: 0.225\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\ntraining...\n------------fold no---------1----------------------\n[CV 1/3; 1/6] END weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294];, score=nan total time=  27.3s\nepoch 1, 512 samples, loss: 3.590, training f1: 0.287, epoch 1, 512 samples, loss: 3.134, training f1: 0.336, validation loss: 1.916, validation f1: 0.121\nnew minimal validation error\nvalidation loss: 1.482, validation f1: 0.188\nnew minimal validation error\n[CV 3/3; 1/6] START weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.946, training f1: 0.314, validation loss: 11.077, validation f1: 0.215\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.761, training f1: 0.280, validation loss: 3.312, validation f1: 0.088\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.765, training f1: 0.276, validation loss: 4.326, validation f1: 0.086\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\n[CV 3/3; 1/6] END weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294];, score=nan total time=  26.8s\nepoch 1, 512 samples, loss: 3.541, training f1: 0.279, validation loss: 5.899, validation f1: 0.137\nnew minimal validation error\n[CV 2/3; 2/6] START weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.085, training f1: 0.366, validation loss: 5.118, validation f1: 0.196\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.086, training f1: 0.297, validation loss: 1.762, validation f1: 0.196\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.323, training f1: 0.336, validation loss: 1.860, validation f1: 0.136\nnew minimal validation error\n[CV 2/3; 2/6] END weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173];, score=nan total time=  26.1s\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.791, training f1: 0.353, validation loss: 5.288, validation f1: 0.144\nnew minimal validation error\n[CV 1/3; 3/6] START weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.312, training f1: 0.276, validation loss: 1.352, validation f1: 0.159\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.830, training f1: 0.296, validation loss: 3.230, validation f1: 0.224\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.463, training f1: 0.277, validation loss: 1.812, validation f1: 0.176\nnew minimal validation error\n[CV 1/3; 3/6] END weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332];, score=nan total time=  25.4s\n[CV 3/3; 3/6] START weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.321, training f1: 0.295, validation loss: 4.172, validation f1: 0.136\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.193, training f1: 0.298, validation loss: 3.021, validation f1: 0.083\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.846, training f1: 0.196, validation loss: 3.131, validation f1: 0.178\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\n[CV 3/3; 3/6] END weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332];, score=nan total time=  27.7s\nepoch 1, 512 samples, loss: 3.526, training f1: 0.240, validation loss: 4.031, validation f1: 0.098\nnew minimal validation error\n[CV 2/3; 4/6] START weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.172, training f1: 0.323, validation loss: 3.538, validation f1: 0.164\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.819, training f1: 0.345, validation loss: 3.643, validation f1: 0.199\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.187, training f1: 0.305, validation loss: 1.913, validation f1: 0.149\nnew minimal validation error\ntraining...\n[CV 2/3; 4/6] END weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458];, score=nan total time=  25.6s\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.902, training f1: 0.351, validation loss: 2.737, validation f1: 0.147\nnew minimal validation error\n[CV 1/3; 5/6] START weights=[0.03958962422796164, 0.11496177586108136, 1.9114621746640914e-05, 0.02277252056746437, 0.09944065951343806, 0.02860901349600653, 0.5011197968279052, 0.027376789035628013, 0.12082936155116308, 0.04528134429760511]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.881, training f1: 0.320, validation loss: 2.516, validation f1: 0.175\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.218, training f1: 0.273, validation loss: 1.456, validation f1: 0.285\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------[CV 2/3; 1/6] START weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294]\ntraining...\n------------fold no---------1----------------------\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.468, training f1: 0.306, epoch 1, 512 samples, loss: 3.149, training f1: 0.289, validation loss: 6.976, validation f1: 0.231\nnew minimal validation error\nvalidation loss: 1.257, validation f1: 0.290\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\n[CV 2/3; 1/6] END weights=[0.014618780486909122, 0.04994350049027696, 0.049850688711501556, 0.03414166901959992, 0.03453600870164408, 0.2828079115983779, 0.020852981596448772, 0.054336644946371915, 0.42381337536508545, 0.035098439083784294];, score=nan total time=  24.9s\nepoch 1, 512 samples, loss: 3.155, training f1: 0.300, validation loss: 3.275, validation f1: 0.157\nnew minimal validation error\n[CV 1/3; 2/6] START weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.401, training f1: 0.258, validation loss: 3.704, validation f1: 0.043\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.213, training f1: 0.325, validation loss: 1.895, validation f1: 0.121\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.740, training f1: 0.273, validation loss: 2.565, validation f1: 0.022\nnew minimal validation error\n[CV 1/3; 2/6] END weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173];, score=nan total time=  26.1s\n[CV 3/3; 2/6] START weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.619, training f1: 0.284, validation loss: 3.529, validation f1: 0.130\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.558, training f1: 0.348, validation loss: 3.065, validation f1: 0.085\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.634, training f1: 0.273, validation loss: 2.864, validation f1: 0.178\nnew minimal validation error\n[CV 3/3; 2/6] END weights=[0.08892882999066232, 0.1602232913705467, 0.020433382458239224, 0.08565364362353511, 0.1257972239235351, 0.07539618918969715, 0.022570125631150906, 0.022460763373684967, 0.04317078461723123, 0.3553657658217173];, score=nan total time=  26.0s\n[CV 2/3; 3/6] START weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.766, training f1: 0.335, validation loss: 2.514, validation f1: 0.106\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.087, training f1: 0.318, validation loss: 2.534, validation f1: 0.296\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.812, training f1: 0.221, validation loss: 2.300, validation f1: 0.109\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\n[CV 2/3; 3/6] END weights=[0.005780237417743139, 0.013519328891973714, 0.12446886128587853, 0.23357901317700103, 0.07604057870389713, 0.019438905870912793, 0.03527500043238585, 0.022059281131798114, 0.4033542911307564, 0.06648450195765332];, score=nan total time=  26.0s\nepoch 1, 512 samples, loss: 3.541, training f1: 0.325, validation loss: 2.993, validation f1: 0.237\nnew minimal validation error\n[CV 1/3; 4/6] START weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.168, training f1: 0.362, validation loss: 2.270, validation f1: 0.226\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.353, training f1: 0.316, validation loss: 2.395, validation f1: 0.237\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.063, training f1: 0.304, validation loss: 1.434, validation f1: 0.181\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\n[CV 1/3; 4/6] END weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458];, score=nan total time=  26.7s\nepoch 1, 512 samples, loss: 2.774, training f1: 0.341, validation loss: 1.431, validation f1: 0.138\nnew minimal validation error\n[CV 3/3; 4/6] START weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.460, training f1: 0.310, validation loss: 5.140, validation f1: 0.193\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.224, training f1: 0.298, validation loss: 6.618, validation f1: 0.069\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 2.661, training f1: 0.240, validation loss: 3.937, validation f1: 0.160\nnew minimal validation error\n[CV 3/3; 4/6] END weights=[0.05417519864614284, 0.0870525657703567, 0.0030179571854087595, 0.019757502140052896, 0.39332701369950696, 0.030198270689601237, 0.18345146338870555, 0.16632711317644977, 0.019626037653180717, 0.04306687765059458];, score=nan total time=  25.9s\n[CV 2/3; 5/6] START weights=[0.03958962422796164, 0.11496177586108136, 1.9114621746640914e-05, 0.02277252056746437, 0.09944065951343806, 0.02860901349600653, 0.5011197968279052, 0.027376789035628013, 0.12082936155116308, 0.04528134429760511]\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.271, training f1: 0.351, validation loss: 2.154, validation f1: 0.276\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.746, training f1: 0.262, validation loss: 3.274, validation f1: 0.108\nnew minimal validation error\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 4.072, training f1: 0.200, validation loss: 1.719, validation f1: 0.153\nnew minimal validation error\n[CV 2/3; 5/6] END weights=[0.03958962422796164, 0.11496177586108136, 1.9114621746640914e-05, 0.02277252056746437, 0.09944065951343806, 0.02860901349600653, 0.5011197968279052, 0.027376789035628013, 0.12082936155116308, 0.04528134429760511];, score=nan total time=  25.1s\ntraining...\n------------fold no---------1----------------------\nepoch 1, 512 samples, loss: 3.394, training f1: 0.309, validation loss: 2.730, validation f1: 0.158\nnew minimal validation error","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n18 fits failed out of a total of 18.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n18 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 324, in fit\n    return super().fit(X, transformed_y, sample_weight)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\", line 83, in fit\n    for idx, clf in enumerate(clfs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 1054, in __call__\n    self.retrieve()\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 933, in retrieve\n    self._output.extend(job.get(timeout=self.timeout))\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 657, in get\n    raise self._value\n  File \"/opt/conda/lib/python3.7/multiprocessing/pool.py\", line 121, in worker\n    result = (True, func(*args, **kwds))\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_base.py\", line 42, in _fit_single_estimator\n    estimator.fit(X, y)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\", line 394, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/tmp/ipykernel_23/136628840.py\", line 601, in fit\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 377, in save\n    with _open_file_like(f, 'wb') as opened_file:\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 231, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 212, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nOSError: [Errno 30] Read-only file system: 'opti_test'\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py:972: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan]\n  category=UserWarning,\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)","\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 431, in _process_worker\n    r = call_item()\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 285, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in __call__\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\", line 263, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/utils/fixes.py\", line 216, in __call__\n    return self.function(*args, **kwargs)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_base.py\", line 42, in _fit_single_estimator\n    estimator.fit(X, y)\n  File \"/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\", line 394, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/tmp/ipykernel_23/136628840.py\", line 601, in fit\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 377, in save\n    with _open_file_like(f, 'wb') as opened_file:\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 231, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/serialization.py\", line 212, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nOSError: [Errno 30] Read-only file system: 'opti_test'\n\"\"\"","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1660868439.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mweights_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'weights'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mweights_grid\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_grid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f1_macro\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mtransformed_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_voting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             )\n\u001b[0;32m---> 83\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"drop\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'opti_test'"],"ename":"OSError","evalue":"[Errno 30] Read-only file system: 'opti_test'","output_type":"error"}]},{"cell_type":"code","source":"predictions = []\nX_size = 0\npred_size = 0\nfor test in test_records:\n    X_test = make_data_set_test(test)\n    size, _ = X_test.shape\n    X_size += size\n    prediction_list = weights_grid.pred(X_test)\n    pred_size += len(prediction_list)\n    record_number = int(test[-5])\n    for i, pred in enumerate(prediction_list):\n        predictions.append({\"identifier\":record_number * 10000 + i,'target':int(pred)})\n    \n\n\npredictions = pd.DataFrame(predictions)\nprint(predictions)\npredictions.to_csv('submission.csv',index = None)\n\nif len(predictions) != 2646:\n    print('WARNING: incorrect submission length! (', len(predictions), ' instead of 2646)')\nelse:\n    print('GOOD TO GO! CORRECT SIZE')","metadata":{"execution":{"iopub.status.busy":"2023-01-02T13:44:05.080427Z","iopub.status.idle":"2023-01-02T13:44:05.081362Z","shell.execute_reply.started":"2023-01-02T13:44:05.081084Z","shell.execute_reply":"2023-01-02T13:44:05.081113Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"https://github.com/Kaggle/kaggle-api","metadata":{}}]}