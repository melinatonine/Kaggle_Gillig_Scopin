{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the challenge begin\n",
    "\n",
    "**Notes on data** \n",
    "\n",
    "- 5 EEG derivations sampled at 250Hz\n",
    "- 3 Accelerometers derivations sampled at 50Hz\n",
    "- Sleep epoch = 30 sec\n",
    "- hypnogram = succession of the sleep stages (0...5)\n",
    "\n",
    "**General info sleep**\n",
    "- Sleep stages = (N1, N2) = light sleep, N3 = deep sleep, REM\n",
    "- Low frequency power: N3 > N2 > N1-REM-Wake\n",
    "\n",
    "**Wake**\n",
    "- During Wake epoch alpha waves are clearly visible on the F-O derivation\n",
    "- Movement occured mainly during wake periods, noisy signals during movement\n",
    "- Alpha wave frequency ranges between 8 and 13 hertz = wake, relaxed\n",
    "**N1**\n",
    "- Theta waves freq betw 4 and 8 Hz = N1, N2\n",
    "\n",
    "**N2**\n",
    "- On N2 epoch, power in the spindle range is much higher on frontal-frontal channels\n",
    "- Theta waves freq betw 4 and 8 Hz = N1, N2\n",
    "- During N2, sleep spindles (fast rythm between 12-14Hz which last between 0.5 up to 2 seconds) are more visible on the Frontal-frontal derivation\n",
    "\n",
    "**N3**\n",
    "- On N3 epoch, we can see more power in the low frequencies\n",
    "- Delta waves freq betw 1 and 4 Hz = N3\n",
    "\n",
    "**REM**\n",
    "- REM sleep distinguishable with steady EEG and eyes movement which can be seen when looking at Frontal-occipital vs frontal-frontal derivation.\n",
    "- The EEG power increases in the low-frequency band when the sleep stage change from REM to NREM sleep stages\n",
    "- REM epoch have more steady EEG\n",
    "\n",
    "**Formulas**\n",
    "- Spectrogram are the time-frequency matrix z = P(t, f)\n",
    "- Spectrum correspond to the curves y = P(frequency)\n",
    "- Average Spectrum can therefore be computed as the mean of spectromgram over a specified period \n",
    "\n",
    "**Links**\n",
    "https://opentext.wsu.edu/psych105/chapter/stages-of-sleep/\n",
    "https://www.sleepfoundation.org/how-sleep-works/alpha-waves-and-sleep\n",
    "https://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/Challenge%20Data%20Dreem-1.pdf\n",
    "https://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/entropy-18-00272.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import yasa\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import iirfilter, filtfilt\n",
    "from scipy.signal import welch\n",
    "from lspopt import spectrogram_lspopt\n",
    "from scipy.signal import spectrogram\n",
    "from os import listdir\n",
    "from random import randint\n",
    "import random as rd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('corr_sleep_stages.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = np.load('sample/sample/f7_O2.npy') \n",
    "print('EEG duration', eeg.shape[0] / 250)\n",
    "accelerometer_x = np.load('sample/sample/accelerometer_x.npy') \n",
    "print('Accelerometer duration', accelerometer_x.shape[0] / 50)\n",
    "hypnogram = np.array(json.load(open('sample/sample/hypnogram.json')))\n",
    "eeg = np.load('sample/sample/f7_O2.npy')\n",
    "eeg_frontal = np.load('sample/sample/f8_f7.npy')\n",
    "accelerometer_x = np.load('sample/sample/accelerometer_x.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_spectrum_for_epochs(eeg,epochs):\n",
    "    \"\"\"\n",
    "    Return the average power in each of the fourier bin for several epochs.\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    psds = []\n",
    "    for epoch in epochs:\n",
    "        idx_start,idx_end = 250 * 30 * epoch,250 * 30 * (epoch + 1)\n",
    "        freqs,t,psd = spectrogram_lspopt(np.clip(eeg[idx_start:idx_end],-150,150),250,nperseg = 1000)\n",
    "        psds += [np.mean(psd ** 2,1)]\n",
    "    return freqs,np.array(psds).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# true_labels = hypnogram\n",
    "\n",
    "# # Get your scores\n",
    "# scores = {}\n",
    "# scores['balanced_accuracy'] = balanced_accuracy_score(true_labels, predictions)\n",
    "# scores['cohen_kappa'] = cohen_kappa_score(true_labels, predictions)\n",
    "# scores['confusion_matrix'] = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot N sleep epochs for a specific stage \n",
    "\n",
    "freq = 250 \n",
    "epoch_s = 30\n",
    "\n",
    "def random_sleep_epoch(N, sleep_stage) :\n",
    "    k = 0\n",
    "    a = randint(0,len(hypnogram))\n",
    "    epochs = []\n",
    "    while k < N:\n",
    "        if hypnogram[a] == sleep_stage :\n",
    "            epochs.append(a)\n",
    "            k += 1\n",
    "            a = randint(0,len(hypnogram))\n",
    "        else :\n",
    "            a = randint(0,len(hypnogram))\n",
    "    eeg_ff = np.load('sample/sample/f8_f7.npy')\n",
    "    for epoch in epochs : \n",
    "        t0 = epoch*epoch_s*freq\n",
    "        eeg_short = eeg_ff[t0:t0+(epoch_s*freq)]\n",
    "        plt.figure(figsize=(25, 8))\n",
    "        plt.plot(eeg_short)\n",
    "        plt.ylim([-200, 200])\n",
    "        plt.xlim(0,len(eeg_short))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bands = {\n",
    "        \"delta\": [0.5, 4],\n",
    "        \"theta\": [4, 8],\n",
    "        \"alpha\": [8, 12],\n",
    "       \"sigma\": [12, 16],\n",
    "       \"beta\": [16, 30]\n",
    "    }\n",
    "\n",
    "EEG_FS = 250\n",
    "ACC_FS = 50 \n",
    "epoch_s = 30\n",
    "n_EEG = 5\n",
    "n_ACC = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_spectral_power_for_epoch(data):\n",
    "    \"\"\"\n",
    "    Compute the relative spectral power for each of the frequency bands defined above\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    psds = []\n",
    "    sfreqs,t,psd = spectrogram(data,250,nperseg = 1000,noverlap = 750)\n",
    "    psd = np.mean(np.abs(psd),-1)\n",
    "    spectral_power_band = {}\n",
    "    for name, freqband in frequency_bands.items():\n",
    "        spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n",
    "        spec_power = np.sum(spec_power, 1)\n",
    "        spectral_power_band[name] = spec_power / np.sum(psd,1)\n",
    "        \n",
    "    return spectral_power_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {\n",
    "    \"stdev\":lambda x:np.std(x,1),\n",
    "    \"mean\":lambda x:np.mean(np.abs(x),1)\n",
    "}\n",
    "variable_lists = list(frequency_bands) + list(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(data):\n",
    "    \"\"\"\n",
    "    Compute the statistics of a signal\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    result = {k:f(data) for k,f in statistics.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_records_all = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    EEG = x[:,1:EEG_FS * epoch_s * n_EEG + 1]\n",
    "    ACC = x[:,EEG_FS * epoch_s * n_EEG + 1:]\n",
    "    EEG = EEG.reshape(len(x), n_EEG, EEG_FS * epoch_s)\n",
    "    ACC = ACC.reshape(len(x), n_ACC, ACC_FS * epoch_s)\n",
    "\n",
    "    for i in range (n_EEG) :\n",
    "        sfreqs,t,psd = spectrogram(EEG[:,i,:], EEG_FS, nperseg = 1000,noverlap = 750)\n",
    "        psd = np.mean(np.abs(psd),-1)\n",
    "        spectral_power_band = []\n",
    "        l = []\n",
    "        for name, freqband in frequency_bands.items():\n",
    "            spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n",
    "            spec_power = np.sum(spec_power, 1)\n",
    "            l.append(spec_power / np.sum(psd,1))\n",
    "        matrice = np.array(l) \n",
    "        matrice = np.vstack((matrice, np.array([np.mean(EEG[k,i,:]) for k in range (len(EEG))]).T))\n",
    "        matrice = np.vstack((matrice, np.array([np.std(EEG[k,i,:]) for k in range (len(EEG))]).T))\n",
    "\n",
    "        if i == 0:\n",
    "            complete_array = matrice \n",
    "        else :\n",
    "            complete_array = np.vstack((complete_array,matrice))\n",
    "    \n",
    "    for i in range (n_ACC) : \n",
    "        sfreqs,t,psd = spectrogram(ACC[:,i,:], ACC_FS, nperseg = 1000,noverlap = 750)\n",
    "        psd = np.mean(np.abs(psd),-1)\n",
    "        spectral_power_band = []\n",
    "        l = []\n",
    "        for name, freqband in frequency_bands.items():\n",
    "            spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n",
    "            spec_power = np.sum(spec_power, 1)\n",
    "            l.append(spec_power / np.sum(psd,1))\n",
    "        matrice = np.array(l)\n",
    "        matrice = np.vstack((matrice, np.array([np.mean(ACC[k,i,:]) for k in range (len(ACC))]).T))\n",
    "        matrice = np.vstack((matrice, np.array([np.std(ACC[k,i,:]) for k in range (len(ACC))]).T))\n",
    "        complete_array = np.vstack((complete_array, matrice))\n",
    "    data_for_records_all[record] = complete_array.T\n",
    "    hypnogram_for_records[record] = list(hypnograms[hypnograms['record'] == record_number]['target'])\n",
    "    \n",
    "variable_lists = (list(frequency_bands) + list(statistics)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_list = []\n",
    "stats = 'mean', 'std'\n",
    "for n in range (n_EEG) :\n",
    "    for freq in list(frequency_bands) : \n",
    "        variable_list.append(f\"EEG_{n+1}_{freq}\")\n",
    "    for stat in stats :\n",
    "        variable_list.append(f\"EEG_{n+1}_{stat}\")\n",
    "for n in range (n_ACC) : \n",
    "    for freq in list(frequency_bands) : \n",
    "        variable_list.append(f\"ACC_{n+1}_{freq}\")\n",
    "    for stat in stats :\n",
    "        variable_list.append(f\"ACC_{n+1}_{stat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(variable_list))\n",
    "print(len(data_for_records_all['record_0.npy'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(record, data, n) :\n",
    "    corr = [0]*n\n",
    "    for i in range (n) :\n",
    "        corr[i] = [0]*n\n",
    "        for j in range (n) :\n",
    "            corr[i][j] = np.corrcoef(data[record,i,:], data[record,j,:])[0][1]\n",
    "    return corr \n",
    "\n",
    "# EEG\n",
    "corr = correlations(0, EEG, n_EEG)\n",
    "for i in range (n_EEG) :\n",
    "    print(i+1,corr[i])\n",
    "\n",
    "# ACC\n",
    "corr = correlations(0, ACC, n_ACC)\n",
    "for i in range (n_ACC) :\n",
    "    print(i+1,corr[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- include all EEG / accelerometer? channels\n",
    "    - for EEG: do spectral analysis for each channel\n",
    "    - think about what to do with accelerometer channels \n",
    "\n",
    "change variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kaggle/kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and cross-validation partitions\n",
    "rd.seed(2134)\n",
    "records_list = list(data_for_records_all)\n",
    "rd.shuffle(records_list)\n",
    "training_record,test_records = records_list[:4],records_list[4:]\n",
    "\n",
    "print('Training records: ',training_record)\n",
    "print('Test records: ', test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY LATER: take training data and test data already cut  / for now keep this division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(records, data_for_records_all,hypnogram_for_records):\n",
    "    X,y = [],[]\n",
    "    for record in records : \n",
    "        y.extend(hypnogram_for_records[record])\n",
    "        X.extend(data_for_records_all[record])\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train = build_dataset(training_record,data_for_records_all,hypnogram_for_records)\n",
    "X_test,y_test = build_dataset(test_records,data_for_records_all,hypnogram_for_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train), len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run decision tree\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "print('training...')\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# test it\n",
    "predictions = clf_rf.predict(X_test)\n",
    "scores = {'balanced_accuracy': balanced_accuracy_score(y_test, predictions),\n",
    "            'cohen_kappa': cohen_kappa_score(y_test, predictions),\n",
    "            'macro_f1': f1_score(y_test, predictions,average ='macro')}\n",
    "\n",
    "print(scores)\n",
    "plot_confusion_matrix(clf_rf, X_test,y_test,display_labels = ['Wake','N1','N2','N3','REM'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "\n",
    "# metrics = \"balanced_accuracy\",\"f1_macro\"\n",
    "\n",
    "# sklearn.model_selection.cross_validate(clf_rf, X_train, y_train, scoring=metrics)\n",
    "\n",
    "# clf_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate importance of each parameter: Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('permutations...')\n",
    "from sklearn.inspection import permutation_importance\n",
    "# We use the build-in sklearn fontion to compute the permutation importance\n",
    "result = permutation_importance(clf_rf, X_test, y_test, n_repeats=50, random_state=0,scoring = 'f1_macro')\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# And plot the importance of each variable\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=variable_list)\n",
    "ax.set_title(\"Permutation Importances (Test set)\")\n",
    "plt.xlabel('Decrease in Macro-F1')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = []\n",
    "for k in range (len(variable_list)) : \n",
    "    X_train_k = X_train.copy()\n",
    "    X_train_k = np.delete(X_train_k, k, 1)\n",
    "    X_test_k = X_test.copy()\n",
    "    X_test_k = np.delete(X_test_k, k, 1)\n",
    "    clf_rf.fit(X_train_k, y_train)\n",
    "    predictions = clf_rf.predict(X_test_k)\n",
    "    f1.append(f1_score(y_test, predictions,average ='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1)\n",
    "f1_order = np.argsort(np.array(f1))\n",
    "f1_sorted = [f1[i] for i in f1_order]\n",
    "print(f1_order)\n",
    "print(f1_sorted)\n",
    "variables_sorted = [variable_list[i] for i in f1_order]\n",
    "print(variables_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(f1)\n",
    "f1_order = np.argsort(np.array(f1))\n",
    "f1_sorted = [f1[i] for i in f1_order]\n",
    "print(f1_order)\n",
    "print(f1_sorted)\n",
    "variables_sorted = [variable_list[i] for i in f1_order]\n",
    "print(variables_sorted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral components have a very marginal effect on classification -> to be improved"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: implement cross validation and compare with no cross validation\n",
    "check if CV useful with random \n",
    "MAJOR CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: plot test error for CV/test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING OH YEAH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (temporary) load single channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_records = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "# load training records\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    # data_for_records[record] = x[:,1:250 * 30 + 1]\n",
    "    data_for_records[record] = x\n",
    "    hypnogram_for_records[record] = list(hypnograms[hypnograms['record'] == record_number]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### use actual training examplars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.seed(1234)\n",
    "records_list = list(data_for_records)\n",
    "rd.shuffle(records_list)\n",
    "training_records,test_records = records_list[:5],records_list[5:]\n",
    "\n",
    "print('Training records: ',training_records)\n",
    "print('Test records: ', test_records)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE CHANGED TO DEFINE AS TEST SET THE TEST SAMPLES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(records, data_for_records,hypnogram_for_records):\n",
    "    X,y = [],[]\n",
    "    for record in records:\n",
    "        X.append(data_for_records[record])\n",
    "        y.extend(hypnogram_for_records[record])\n",
    "\n",
    "    return np.concatenate(X),y\n",
    "\n",
    "\n",
    "X_train,y_train = build_dataset(training_records,data_for_records,hypnogram_for_records)\n",
    "X_test,y_test = build_dataset(test_records,data_for_records,hypnogram_for_records)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define load functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load project data\n",
    "    DataLoader and Dataset for single-channel EEG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def normalize_data(eeg_array):\n",
    "    \"\"\"normalize signal between 0 and 1\"\"\"\n",
    "\n",
    "    normalized_array = np.clip(eeg_array, -250, 250)\n",
    "    normalized_array = normalized_array / 250\n",
    "\n",
    "    return normalized_array\n",
    "\n",
    "\n",
    "class EegEpochDataset(Dataset):\n",
    "    \"\"\"EEG Epochs dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_data (numpy array): Numpy array of input data.\n",
    "            y_data (list of numpy array): Sleep Stages\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.y_data = y_data\n",
    "        self.x_data = x_data\n",
    "        self.transform = transform\n",
    "\n",
    "        self.x_data = normalize_data(x_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        signal = np.expand_dims(self.x_data[idx], axis=0)\n",
    "        stage = self.y_data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, stage\n",
    "\n",
    "\n",
    "training_dataset = EegEpochDataset(X_train,y_train)\n",
    "training_dataloader = DataLoader(training_dataset,batch_size = 32)\n",
    "# validation_dataset = EegEpochDataset(X_test,y_test)\n",
    "# validation_dataloader = DataLoader(validation_dataset,batch_size = 32)\n",
    "test_dataset = EegEpochDataset(X_test,y_test)\n",
    "test_dataloader = DataLoader(test_dataset,batch_size = 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First CNN model\n",
    "+ max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "# class SingleChannelConvNet(nn.Module):\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super(SingleChannelConvNet, self).__init__()\n",
    "#         self.conv_a = nn.Conv1d(1, 8, 25, stride=5)\n",
    "#         self.conv_b = nn.Conv1d(8, 16, 10, stride=5)\n",
    "#         self.conv_c = nn.Conv1d(16, 32, 10, stride=5)\n",
    "#         self.conv_d = nn.Conv1d(32, 64, 10, stride=5)\n",
    "\n",
    "#         self.relu = nn.ReLU()\n",
    "\n",
    "#         self.fc1 = nn.Linear(64, 5)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv_a(x)\n",
    "#         print(x.shape)\n",
    "#         x = self.relu(x)\n",
    "#         # x = self.relu(self.conv_a(x))\n",
    "#         x = self.relu(self.conv_b(x))\n",
    "#         x = self.relu(self.conv_c(x))\n",
    "#         x = self.relu(self.conv_d(x))\n",
    "#         x = x.max(-1)[0]\n",
    "#         x = self.fc1(x)\n",
    "\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleChannelConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SingleChannelConvNet, self).__init__()\n",
    "        self.conv_a = nn.Conv1d(1, 8, 25, stride=5)\n",
    "        self.batchnorm_a = nn.BatchNorm1d(8, eps=0.001, momentum=0.99)\n",
    "        self.conv_b = nn.Conv1d(8, 32, 10, stride=3)\n",
    "        self.batchnorm_b = nn.BatchNorm1d(32, eps=0.001, momentum=0.99)\n",
    "        self.conv_c = nn.Conv1d(32, 64, 10, stride=3)\n",
    "        self.batchnorm_c= nn.BatchNorm1d(64, eps=0.001, momentum=0.99)\n",
    "        self.conv_d = nn.Conv1d(64, 128, 10, stride=2)\n",
    "        self.batchnorm_d= nn.BatchNorm1d(128, eps=0.001, momentum=0.99)\n",
    "        self.conv_e = nn.Conv1d(128, 256, 10, stride=1)\n",
    "        self.batchnorm_e= nn.BatchNorm1d(256, eps=0.001, momentum=0.99)\n",
    "       \n",
    "\n",
    "        self.max_pool = nn.MaxPool1d(1, 1)\n",
    "        # Size of a layer after convolution : (-W - F + 2P)/S +1 \n",
    "        # / size of conv a after max pool : (8 - 2)/1 +1 = 7\n",
    "        # self.avg_pool=torch.nn.AvgPool1d(kernel_size=256)\n",
    "    \n",
    "        \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        self.fc1 = nn.Linear(256, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.batchnorm_a(self.relu(self.conv_a(x)))\n",
    "        x = self.batchnorm_b(self.relu(self.conv_b(x)))\n",
    "        x = self.batchnorm_c(self.relu(self.conv_c(x)))\n",
    "        x = self.batchnorm_d(self.relu(self.conv_d(x)))\n",
    "        x = self.batchnorm_e(self.relu(self.conv_e(x)))\n",
    "        x = x.max(-1)[0]\n",
    "        x = self.fc1(x)\n",
    "        # x = self.avg_pool(x)\n",
    "        # x = self.max_pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        \n",
    "        # print(x.shape)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  X_train.shape : (4286, 42001)\n",
    "# shape after conv a : torch.Size([32, 8, 8396])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement early stopping to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from skorch import NeuralNetClassifier\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "\n",
    "# device: use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parameters\n",
    "n_epoch = 50\n",
    "learning_rate = 1e-3\n",
    "\n",
    "min_validation_loss = np.inf\n",
    "\n",
    "\n",
    "# neural network and co\n",
    "my_net = SingleChannelConvNet()\n",
    "my_net = my_net.to(device) # model into GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(my_net.parameters())\n",
    "my_net.train()\n",
    "print('training...')\n",
    "\n",
    "#KFold setting\n",
    "KCV = KFold(n_splits=6) #shuffle=True, random_state=15011999\n",
    "Kidx = KCV.get_n_splits(X_train)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(KCV.split(X_train)):\n",
    "    print('------------fold no---------{}----------------------'.format(fold))\n",
    "\n",
    "    early_stopper = EarlyStopper(patience=6, min_delta=0.05)\n",
    "\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index)\n",
    "    val_subsampler = torch.utils.data.SubsetRandomSampler(val_index)\n",
    "\n",
    "    training_dataloader = DataLoader(training_dataset, sampler=train_subsampler, batch_size = 32)\n",
    "    validation_dataloader = DataLoader(training_dataset, sampler=val_subsampler, batch_size = 32)\n",
    "\n",
    "    \n",
    "    for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(training_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = my_net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print('epoch %d, %d samples, loss: %.3f' % (epoch + 1, (i+1)*training_dataloader.batch_size,running_loss / (i+1)), end = \", \")\n",
    "        \n",
    "        running_loss = 0.0\n",
    "\n",
    "        #ADD CROSSVALIDATION\n",
    "        my_net.eval()\n",
    "        validation_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(validation_dataloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "            # forward\n",
    "            outputs = my_net.forward(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            validation_loss += loss.item()\n",
    "\n",
    "            # print statistics\n",
    "        print('validation loss: %.3f' % (validation_loss / (i+1)))\n",
    "\n",
    "        if validation_loss < min_validation_loss:\n",
    "            min_validation_loss = validation_loss\n",
    "            print('new minimal validation error')\n",
    "            torch.save(my_net.state_dict(), 'my_net_opti')\n",
    "\n",
    "        \n",
    "        if early_stopper.early_stop(validation_loss):    \n",
    "            print('aie at epoch', epoch)         \n",
    "            break\n",
    "            \n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTING ADABOOST CNN https://www.sciencedirect.com/science/article/abs/pii/S0925231220304379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "# params\n",
    "classes = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
    "\n",
    "# Load saved parameters dictionnary\n",
    "my_net.load_state_dict(torch.load('my_net_opti'))\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_list = torch.empty(0).to(device)\n",
    "    true_list = torch.empty(0).to(device)\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "        \n",
    "        outputs = my_net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_list = torch.cat([prediction_list, predicted])\n",
    "        true_list = torch.cat([true_list, labels])\n",
    "\n",
    "true_list = true_list.cpu().numpy()\n",
    "prediction_list = prediction_list.cpu().numpy()\n",
    "scores = {'balanced_accuracy': balanced_accuracy_score(true_list, prediction_list),\n",
    "            'macro_f1': f1_score(true_list, prediction_list, average = 'macro'),\n",
    "            'confusion_matrix': confusion_matrix(true_list, prediction_list)}\n",
    "\n",
    "for elt in scores:\n",
    "    print(elt)\n",
    "    print(scores[elt])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement k-fold cross validation\n",
    "* implement Ensemble with 1 CNN for each EEG/accelerometer channel (raw data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement weighted average ensemble:\n",
    "* weight the prediction of each CNN according to its validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_net = NeuralNetClassifier(SingleChannelConvNet)\n",
    "new_net.initialize()  # This is important!\n",
    "new_net.load_params(f_params='my_net_opti')\n",
    "# X_train = X_train.astype(np.float32)\n",
    "# y_train = np.array(y_train, dtype='int64')\n",
    "for i, data in enumerate(training_dataloader, 0):\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "    # inputs, labels = np.array(inputs), np.array(labels, dtype=int)\n",
    "    inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "    new_net.fit(inputs,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import ParameterGrid, GridSearchCV\n",
    "# n_CNN = n_EEG\n",
    "# n_CNN = 1\n",
    "\n",
    "# weights_grid = ParameterGrid({'weights': [[np.linspace(0,1,9)] for i in range(n_CNN)]})\n",
    "# weights_grid = ParameterGrid({'weights': [[0, 1]]})\n",
    "weights_grid = {'weights': [[0.5, 1], [1, 0.5], [1, 1]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[np.linspace(0,1,9)] for i in range(n_CNN)]\n",
    "[[0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vc = VotingClassifier(estimators=[('cnn', NeuralNetClassifier(SingleChannelConvNet().load_state_dict(torch.load('my_net_opti')))), ('rf',RandomForestClassifier())],\n",
    "#                    voting='soft',n_jobs=-1)\n",
    "vc = VotingClassifier(estimators=[('cnn', new_net), ('rf',RandomForestClassifier())],\n",
    "                   voting='soft',n_jobs=-1)\n",
    "\n",
    "grid_Search = GridSearchCV(param_grid = weights_grid, estimator=vc, refit=False, scoring='f1_macro', verbose=2)\n",
    "new_net.set_params(train_split=False, verbose=0)\n",
    "\n",
    "# for i, data in enumerate(validation_dataloader, 0):\n",
    "#     # get the inputs; data is a list of [inputs, labels]\n",
    "#     inputs, labels = data\n",
    "#     inputs, labels = np.array(inputs), np.array(labels, dtype=int)\n",
    "#     # inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "grid_Search.fit(X_train, y_train)\n",
    "\n",
    "# print(grid_Search.best_Score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def your_function(number):\n",
    "    print(number)\n",
    "\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "param_grid = {'param1': [1, 2, 3]}\n",
    "\n",
    "grid = ParameterGrid(param_grid)\n",
    "\n",
    "for params in grid:\n",
    "    your_function(params['param1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RepeatedStratifiedKFold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://machinelearningmastery.com/stacking-ensemble-machine-learning-with-python/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting to the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for record in os.listdir(\"test_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    # x = np.load(f'test_records/{record}')\n",
    "    # raw_data_for_record = x[:,1:250 * 30 + 1]\n",
    "    # data_for_record = get_relative_spectral_power_for_epoch(raw_data_for_record)\n",
    "    # data_for_record.update(compute_stats(raw_data_for_record))\n",
    "    # data_for_record = np.array([value for value in data_for_record.values()]).T\n",
    "    # preds = pipeline.predict(data_for_record)\n",
    "    for i, pred in enumerate(prediction_list):\n",
    "        predictions.append({\"identifier\":record_number * 10000 + i,'target':int(prediction_list[i])})\n",
    "\n",
    "predictions = pd.DataFrame(predictions)\n",
    "print(predictions)\n",
    "predictions.to_csv('submission.csv',index = None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03897435e42acb5a1e8b7ee7fe5c61aa484a2cee8fb08f5df3c29da6aaaf0e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
