{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let the challenge begin\n",
    "\n",
    "**Notes on data** \n",
    "\n",
    "- 5 EEG derivations sampled at 250Hz\n",
    "- 3 Accelerometers derivations sampled at 50Hz\n",
    "- Sleep epoch = 30 sec\n",
    "- hypnogram = succession of the sleep stages (0...5)\n",
    "\n",
    "**General info sleep**\n",
    "- Sleep stages = (N1, N2) = light sleep, N3 = deep sleep, REM\n",
    "- Low frequency power: N3 > N2 > N1-REM-Wake\n",
    "\n",
    "**Wake**\n",
    "- During Wake epoch alpha waves are clearly visible on the F-O derivation\n",
    "- Movement occured mainly during wake periods, noisy signals during movement\n",
    "- Alpha wave frequency ranges between 8 and 13 hertz = wake, relaxed\n",
    "**N1**\n",
    "- Theta waves freq betw 4 and 8 Hz = N1, N2\n",
    "\n",
    "**N2**\n",
    "- On N2 epoch, power in the spindle range is much higher on frontal-frontal channels\n",
    "- Theta waves freq betw 4 and 8 Hz = N1, N2\n",
    "- During N2, sleep spindles (fast rythm between 12-14Hz which last between 0.5 up to 2 seconds) are more visible on the Frontal-frontal derivation\n",
    "\n",
    "**N3**\n",
    "- On N3 epoch, we can see more power in the low frequencies\n",
    "- Delta waves freq betw 1 and 4 Hz = N3\n",
    "\n",
    "**REM**\n",
    "- REM sleep distinguishable with steady EEG and eyes movement which can be seen when looking at Frontal-occipital vs frontal-frontal derivation.\n",
    "- The EEG power increases in the low-frequency band when the sleep stage change from REM to NREM sleep stages\n",
    "- REM epoch have more steady EEG\n",
    "\n",
    "**Formulas**\n",
    "- Spectrogram are the time-frequency matrix z = P(t, f)\n",
    "- Spectrum correspond to the curves y = P(frequency)\n",
    "- Average Spectrum can therefore be computed as the mean of spectromgram over a specified period \n",
    "\n",
    "**Links**\n",
    "https://opentext.wsu.edu/psych105/chapter/stages-of-sleep/\n",
    "https://www.sleepfoundation.org/how-sleep-works/alpha-waves-and-sleep\n",
    "https://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/Challenge%20Data%20Dreem-1.pdf\n",
    "https://centralesupelec.edunao.com/pluginfile.php/242107/course/section/36663/entropy-18-00272.pdf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import yasa\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.signal import iirfilter, filtfilt\n",
    "from scipy.signal import welch\n",
    "from lspopt import spectrogram_lspopt\n",
    "from scipy.signal import spectrogram\n",
    "from os import listdir\n",
    "from random import randint\n",
    "import random as rd\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(plt.imread('corr_sleep_stages.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg = np.load('sample/sample/f7_O2.npy') \n",
    "print('EEG duration', eeg.shape[0] / 250)\n",
    "accelerometer_x = np.load('sample/sample/accelerometer_x.npy') \n",
    "print('Accelerometer duration', accelerometer_x.shape[0] / 50)\n",
    "hypnogram = np.array(json.load(open('sample/sample/hypnogram.json')))\n",
    "eeg = np.load('sample/sample/f7_O2.npy')\n",
    "eeg_frontal = np.load('sample/sample/f8_f7.npy')\n",
    "accelerometer_x = np.load('sample/sample/accelerometer_x.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_spectrum_for_epochs(eeg,epochs):\n",
    "    \"\"\"\n",
    "    Return the average power in each of the fourier bin for several epochs.\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    psds = []\n",
    "    for epoch in epochs:\n",
    "        idx_start,idx_end = 250 * 30 * epoch,250 * 30 * (epoch + 1)\n",
    "        freqs,t,psd = spectrogram_lspopt(np.clip(eeg[idx_start:idx_end],-150,150),250,nperseg = 1000)\n",
    "        psds += [np.mean(psd ** 2,1)]\n",
    "    return freqs,np.array(psds).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# true_labels = hypnogram\n",
    "\n",
    "# # Get your scores\n",
    "# scores = {}\n",
    "# scores['balanced_accuracy'] = balanced_accuracy_score(true_labels, predictions)\n",
    "# scores['cohen_kappa'] = cohen_kappa_score(true_labels, predictions)\n",
    "# scores['confusion_matrix'] = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "# print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to plot N sleep epochs for a specific stage \n",
    "\n",
    "freq = 250 \n",
    "epoch_s = 30\n",
    "\n",
    "def random_sleep_epoch(N, sleep_stage) :\n",
    "    k = 0\n",
    "    a = randint(0,len(hypnogram))\n",
    "    epochs = []\n",
    "    while k < N:\n",
    "        if hypnogram[a] == sleep_stage :\n",
    "            epochs.append(a)\n",
    "            k += 1\n",
    "            a = randint(0,len(hypnogram))\n",
    "        else :\n",
    "            a = randint(0,len(hypnogram))\n",
    "    eeg_ff = np.load('sample/sample/f8_f7.npy')\n",
    "    for epoch in epochs : \n",
    "        t0 = epoch*epoch_s*freq\n",
    "        eeg_short = eeg_ff[t0:t0+(epoch_s*freq)]\n",
    "        plt.figure(figsize=(25, 8))\n",
    "        plt.plot(eeg_short)\n",
    "        plt.ylim([-200, 200])\n",
    "        plt.xlim(0,len(eeg_short))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2: Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_bands = {\n",
    "        \"delta\": [0.5, 4],\n",
    "        \"theta\": [4, 8],\n",
    "        \"alpha\": [8, 12],\n",
    "       \"sigma\": [12, 16],\n",
    "       \"beta\": [16, 30]\n",
    "    }\n",
    "\n",
    "EEG_FS = 250\n",
    "ACC_FS = 50 \n",
    "epoch_s = 30\n",
    "n_EEG = 5\n",
    "n_ACC = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = {\n",
    "    \"stdev\":lambda x:np.std(x,1),\n",
    "    \"mean\":lambda x:np.mean(np.abs(x),1)\n",
    "}\n",
    "variable_lists = list(frequency_bands) + list(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_spectral_power_for_epoch(data):\n",
    "    \"\"\"\n",
    "    Compute the relative spectral power for each of the frequency bands defined above\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    psds = []\n",
    "    sfreqs,t,psd = spectrogram(data,250,nperseg = 1000,noverlap = 750)\n",
    "    psd = np.mean(np.abs(psd),-1)\n",
    "    spectral_power_band = {}\n",
    "    for name, freqband in frequency_bands.items():\n",
    "        spec_power = psd[:,(sfreqs >= freqband[0]) & (sfreqs < freqband[1])]\n",
    "        spec_power = np.sum(spec_power, 1)\n",
    "        spectral_power_band[name] = spec_power / np.sum(psd,1)\n",
    "    return spectral_power_band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(data):\n",
    "    \"\"\"\n",
    "    Compute the statistics of a signal\n",
    "    \"\"\"\n",
    "    EEG_FS = 250\n",
    "    result = {k:f(data) for k,f in statistics.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pippo = np.load('./training_records/record_1.npy')\n",
    "np.shape(pippo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_records = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    data_for_record = x[:,1:250 * 30 + 1]\n",
    "    data_for_records[record] = get_relative_spectral_power_for_epoch(data_for_record)\n",
    "    data_for_records[record].update(compute_stats(data_for_record))\n",
    "    hypnogram_for_records[record] = list(hypnograms[hypnograms['record'] == record_number]['target'])\n",
    "    \n",
    "\n",
    "\n",
    "variable_list = list(data_for_records[record].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_records = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    EEG = x[:,1:EEG_FS * epoch_s * n_EEG + 1]\n",
    "    ACC = x[:,EEG_FS * epoch_s * n_EEG + 1:]\n",
    "    EEG = EEG.reshape(len(x), n_EEG, EEG_FS * epoch_s)\n",
    "    ACC = ACC.reshape(len(x), n_ACC, ACC_FS * epoch_s)\n",
    "    data_for_records[record] = {}\n",
    "    hypnogram_for_records[record] = {}\n",
    "    for i in range (n_EEG) :\n",
    "        data_for_records[record][f\"EEG_{i+1}\"] = get_relative_spectral_power_for_epoch(EEG[:,i,:])\n",
    "        data_for_records[record][f\"EEG_{i+1}\"].update(compute_stats(EEG[:,i,:]))\n",
    "        hypnogram_for_records[record][f\"EEG_{i+1}\"] = list(hypnograms[hypnograms['record'] == record_number]['target'])\n",
    "    for i in range (n_ACC) : \n",
    "        data_for_records[record][f\"ACC_{i+1}\"] = get_relative_spectral_power_for_epoch(ACC[:,i,:])\n",
    "        data_for_records[record][f\"ACC_{i+1}\"].update(compute_stats(ACC[:,i,:]))\n",
    "        hypnogram_for_records[record][f\"ACC_{i+1}\"] = list(hypnograms[hypnograms['record'] == record_number]['target'])\n",
    "    \n",
    "\n",
    "variable_list = list(data_for_records[record].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlations(record, data, n) :\n",
    "    corr = [0]*n\n",
    "    for i in range (n) :\n",
    "        corr[i] = [0]*n\n",
    "        for j in range (n) :\n",
    "            corr[i][j] = np.corrcoef(data[record,i,:], data[record,j,:])[0][1]\n",
    "    return corr \n",
    "\n",
    "# EEG\n",
    "corr = correlations(0, EEG, n_EEG)\n",
    "for i in range (n_EEG) :\n",
    "    print(i+1,corr[i])\n",
    "\n",
    "# ACC\n",
    "corr = correlations(0, ACC, n_ACC)\n",
    "for i in range (n_ACC) :\n",
    "    print(i+1,corr[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- include all EEG / accelerometer? channels\n",
    "    - for EEG: do spectral analysis for each channel\n",
    "    - think about what to do with accelerometer channels \n",
    "\n",
    "change variable names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Kaggle/kaggle-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and cross-validation partitions\n",
    "rd.seed(1234)\n",
    "records_list = list(data_for_records)\n",
    "rd.shuffle(records_list)\n",
    "training_record,test_records = records_list[:4],records_list[4:]\n",
    "\n",
    "print('Training records: ',training_record)\n",
    "print('Test records: ', test_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VERY LATER: take training data and test data already cut  / for now keep this division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(records, data_for_records,hypnogram_for_records):\n",
    "    X,y = [],[]\n",
    "    for record in records:\n",
    "        X_for_record = np.expand_dims(np.array([value for value in data_for_records[record].values()]).T,0)\n",
    "        y.extend(hypnogram_for_records[record])\n",
    "        X.extend(X_for_record)\n",
    "    return np.concatenate(X),y\n",
    "\n",
    "\n",
    "X_train,y_train = build_dataset(training_record,data_for_records,hypnogram_for_records)\n",
    "X_test,y_test = build_dataset(test_records,data_for_records,hypnogram_for_records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run decision tree\n",
    "clf_rf = RandomForestClassifier(random_state=42)\n",
    "print('training...')\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "# test it\n",
    "predictions = clf_rf.predict(X_test)\n",
    "scores = {'balanced_accuracy': balanced_accuracy_score(y_test, predictions),\n",
    "            'cohen_kappa': cohen_kappa_score(y_test, predictions),\n",
    "            'macro_f1': f1_score(y_test, predictions,average ='macro')}\n",
    "\n",
    "print(scores)\n",
    "plot_confusion_matrix(clf_rf, X_test,y_test,display_labels = ['Wake','N1','N2','N3','REM'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "\n",
    "# metrics = \"balanced_accuracy\",\"f1_macro\"\n",
    "\n",
    "# sklearn.model_selection.cross_validate(clf_rf, X_train, y_train, scoring=metrics)\n",
    "\n",
    "# clf_rf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate importance of each parameter: Permutation importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('permutations...')\n",
    "from sklearn.inspection import permutation_importance\n",
    "# We use the build-in sklearn fontion to compute the permutation importance\n",
    "result = permutation_importance(clf_rf, X_test, y_test, n_repeats=50, random_state=0,scoring = 'f1_macro')\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "\n",
    "# And plot the importance of each variable\n",
    "fig, ax = plt.subplots(figsize=(25, 10))\n",
    "ax.boxplot(result.importances[sorted_idx].T,\n",
    "           vert=False, labels=variable_list)\n",
    "ax.set_title(\"Permutation Importances (Test set)\")\n",
    "plt.xlabel('Decrease in Macro-F1')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spectral components have a very marginal effect on classification -> to be improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo: implement cross validation and compare with no cross validation\n",
    "check if CV useful with random \n",
    "MAJOR CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: plot test error for CV/test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEEP LEARNING OH YEAH"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (temporary) load single channel data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_records = {}\n",
    "hypnogram_for_records = {}\n",
    "hypnograms = pd.read_csv('targets_train.csv')\n",
    "for record in os.listdir(\"training_records\"):\n",
    "    record_number = int(record[-5])\n",
    "    x = np.load(f'training_records/{record}')\n",
    "    data_for_records[record] = x[:,1:250 * 30 + 1]\n",
    "    hypnogram_for_records[record] = list(hypnograms[hypnograms['record'] == record_number]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd.seed(1234)\n",
    "records_list = list(data_for_records)\n",
    "rd.shuffle(records_list)\n",
    "training_record,test_records = records_list[:5],records_list[5:]\n",
    "\n",
    "print('Training records: ',training_record)\n",
    "print('Test records: ', test_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(records, data_for_records,hypnogram_for_records):\n",
    "    X,y = [],[]\n",
    "    for record in records:\n",
    "        X.append(data_for_records[record])\n",
    "        y.extend(hypnogram_for_records[record])\n",
    "\n",
    "    return np.concatenate(X),y\n",
    "\n",
    "\n",
    "X_train,y_train = build_dataset(training_record,data_for_records,hypnogram_for_records)\n",
    "X_test,y_test = build_dataset(test_records,data_for_records,hypnogram_for_records)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define load functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Load project data\n",
    "    DataLoader and Dataset for single-channel EEG\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "def normalize_data(eeg_array):\n",
    "    \"\"\"normalize signal between 0 and 1\"\"\"\n",
    "\n",
    "    normalized_array = np.clip(eeg_array, -250, 250)\n",
    "    normalized_array = normalized_array / 250\n",
    "\n",
    "    return normalized_array\n",
    "\n",
    "\n",
    "class EegEpochDataset(Dataset):\n",
    "    \"\"\"EEG Epochs dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, x_data, y_data, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x_data (numpy array): Numpy array of input data.\n",
    "            y_data (list of numpy array): Sleep Stages\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.y_data = y_data\n",
    "        self.x_data = x_data\n",
    "        self.transform = transform\n",
    "\n",
    "        self.x_data = normalize_data(x_data)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        signal = np.expand_dims(self.x_data[idx], axis=0)\n",
    "        stage = self.y_data[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            signal = self.transform(signal)\n",
    "\n",
    "        return signal, stage\n",
    "\n",
    "\n",
    "training_dataset = EegEpochDataset(X_train,y_train)\n",
    "training_dataloader = DataLoader(training_dataset,batch_size = 32)\n",
    "validation_dataset = EegEpochDataset(X_test,y_test)\n",
    "validation_dataloader = DataLoader(validation_dataset,batch_size = 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First CNN model\n",
    "+ max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class SingleChannelConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SingleChannelConvNet, self).__init__()\n",
    "        self.conv_a = nn.Conv1d(1, 8, 25, stride=5)\n",
    "        self.conv_b = nn.Conv1d(8, 16, 10, stride=5)\n",
    "        self.conv_c = nn.Conv1d(16, 32, 10, stride=5)\n",
    "        self.conv_d = nn.Conv1d(32, 64, 10, stride=5)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(64, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.relu(self.conv_a(x))\n",
    "        x = self.relu(self.conv_b(x))\n",
    "        x = self.relu(self.conv_c(x))\n",
    "        x = self.relu(self.conv_d(x))\n",
    "        x = x.max(-1)[0]\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* implement early stopping to avoid overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/71998978/early-stopping-in-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# device: use GPU if available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# parameters\n",
    "n_epoch = 80\n",
    "learning_rate = 1e-3\n",
    "\n",
    "early_stopper = EarlyStopper(patience=3, min_delta=10)\n",
    "\n",
    "# neural network and co\n",
    "my_net = SingleChannelConvNet()\n",
    "my_net = my_net.to(device) # model into GPU\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(my_net.parameters())\n",
    "my_net.train()\n",
    "print('training...')\n",
    "for epoch in range(n_epoch):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training_dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = my_net.forward(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "    print('epoch %d, %d samples, loss: %.3f' % (epoch + 1, (i+1)*training_dataloader.batch_size,running_loss / (i+1)))\n",
    "    if early_stopper.early_stop(running_loss):             \n",
    "        break\n",
    "    running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score, confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix, f1_score\n",
    "# params\n",
    "classes = ['Wake', 'N1', 'N2', 'N3', 'REM']\n",
    "\n",
    "with torch.no_grad():\n",
    "    prediction_list = torch.empty(0).to(device)\n",
    "    true_list = torch.empty(0).to(device)\n",
    "    for data in validation_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device).float(), labels.to(device)\n",
    "        \n",
    "        outputs = my_net(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        prediction_list = torch.cat([prediction_list, predicted])\n",
    "        true_list = torch.cat([true_list, labels])\n",
    "\n",
    "true_list = true_list.cpu().numpy()\n",
    "prediction_list = prediction_list.cpu().numpy()\n",
    "scores = {'balanced_accuracy': balanced_accuracy_score(true_list, prediction_list),\n",
    "            'macro_f1': f1_score(true_list, prediction_list, average = 'macro'),\n",
    "            'confusion_matrix': confusion_matrix(true_list, prediction_list)}\n",
    "\n",
    "for elt in scores:\n",
    "    print(elt)\n",
    "    print(scores[elt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "03897435e42acb5a1e8b7ee7fe5c61aa484a2cee8fb08f5df3c29da6aaaf0e3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
